{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPORT ACTIVITATE IUNIE 2017-SEPTEMBRIE 2017\n",
    "\n",
    "- <i><b><a href=\"mailto:laurentiu.piciu@4esoft.ro\" style=\"color: #848484\">Laurentiu-Gheorghe PICIU</a></b><font color='#848484'> (4E Software & Facultatea de Automatică și Calculatoare - UPB)</i> <br>\n",
    "- <i><b><a href=\"mailto:andrei.simion@4esoft.ro\" style=\"color: #848484\">Andrei SIMION-CONSTANTINESCU</a></b><font color='#848484'> (4E Software & Facultatea de Automatică și Calculatoare - UPB)</i>\n",
    "\n",
    "## Prezentare generală\n",
    "\n",
    "În domeniul Învățării Automate (Machine Learning), orice problemă are ca punct de plecare existența unor date. O pereche de forma $(x^{(i)}, y^{(i)})$ constituie un exemplu din setul de date, unde $x^{(i)}$ reprezintă variabilele de intrare (predictori), iar $y^{(i)}$ reprezintă variabila target. Astfel, dându-se un set de date, scopul este să se găsească o funcție $h: X \\rightarrow Y$, cu proprietatea ca $h(x)$ este un predictor bun pentru valoarea corespunzatoare - $y$. În momentul în care varibila target este continuă, atunci interacționăm cu o problemă de **regresie**. Altfel, dacă $y$ aparține unei mulțimi cu un număr mic de valori discrete, atunci problema este una de **clasificare**.\n",
    "\n",
    "Atât în cazul regresiei liniare, cât și în cazul regresiei logistice (clasificare), se dorește găsirea unui **hiperplan** care să aproximeze cât mai bine punctele din setul de date. Ecuația hiperplanului este data de înmulțirea dintre parametrii $\\theta$ (pe care dorim să îi îmbunătățim prin antrenament) și predictori:\n",
    "\n",
    "$h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n} = \\begin{bmatrix} \\theta_{0} &  \\theta_{1} & \\theta_{2} & ... & \\theta_{n}\\end{bmatrix}\\begin{bmatrix} x_{0} \\\\ x_{1} \\\\ x_{2} \\\\ .. \\\\ x_{n}\\end{bmatrix} = \\theta^Tx$, unde $n = $numărul predictorilor și $x_{0} = 1$.\n",
    "\n",
    "1. <h6>Problema clasificării</h6>\n",
    "Pentru ca funcția ipoteză să întoarcă valori în intervalul $[0, 1]$ s-a introdus funcția **sigmoid** $g(z) = \\frac{1}{1 + e^{-z}} \\implies h_{\\theta}(x) = g(\\theta^Tx)$.\n",
    "![alt text](sigmoid.gif \"Graficul funcției sigmoid.\")\n",
    "\n",
    "    Sigmoid-ul este de ajutor pentru clasificare binară, dar și pentru clasificare multinomială **One-vs-All**. Pentru generalizarea clasificării multinomiale, se folosește funcția **softmax**, care este o generalizare a sigmoid-ului. Pentru a putea fi folosit softmax-ul, trebuie ca variabila target sa fie **one-hot**, astfel încât funcția softmax să ofere K probabilități (K = numărul de clase):\n",
    "    \n",
    "    $h_{\\theta}(x^{(i)}) = \\begin{bmatrix} p(y^{(i)} = 1\\,|\\,x^{(i)}; \\theta) \\\\ p(y^{(i)} = 2\\,|\\,x^{(i)}; \\theta)  \\\\ .. \\\\ p(y^{(i)} = k\\,|\\,x^{(i)}; \\theta)\\end{bmatrix} = \\frac{1}{\\sum_{j=1}^{k}e^{\\theta_j^Tx^{(i)}}}\\begin{bmatrix} e^{\\theta_1^Tx^{(i)}} \\\\ e^{\\theta_2^Tx^{(i)}}  \\\\ .. \\\\ e^{\\theta_k^Tx^{(i)}}\\end{bmatrix}$\n",
    "\n",
    "2. <h6>Funcția de cost</h6>\n",
    "Măsurarea performanțelor unui model se face cu ajutorul **funcției de cost**. Pentru clasificare se folosește funcția de cost numită **cross-entropy**.\n",
    "\n",
    "    $J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}Cost(h_{\\theta}(x^{(i)}), y^{(i)})$, unde $m = $numărul de observații. <br>\n",
    "    $Cost(h_{\\theta}(x), y) = -log(h_{\\theta}(x))$, dacă $y = 1$ <br>\n",
    "    $Cost(h_{\\theta}(x), y) = -log(1 - h_{\\theta}(x))$, dacă $y = 0$ <br><br>\n",
    "    $\\implies J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)})log(1 - h_{\\theta}(x^{(i)}))]$\n",
    "\n",
    "2. <h6>Tunarea parametrilor</h6>\n",
    "Pentru îmbunătățirea parametrilor, se va ține cont de derivatele parțiale ale funcției de cost în funcție de fiecare parametru $\\theta_j$, aplicându-se metoda **gradientului descendent**, care modifică parametrii astfel încât funcția de cost să descrească.\n",
    "\n",
    "    $Repeta\\;\\{ \\\\\n",
    "    \\quad \\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)\\\\\n",
    "    \\}$ <br><br>\n",
    "    \n",
    "    $Repeta\\;\\{ \\\\\n",
    "    \\quad \\theta_j := \\theta_j - \\frac{\\alpha}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} \\\\\n",
    "    \\}$\n",
    "    \n",
    "    Varianta vectorizată: $\\theta := \\theta - \\frac{\\alpha}{m}X^T \\cdot residual$\n",
    "\n",
    "În această perioadă, activitatea noastră a avut ca scop înțelegerea conceptelor care stau în spatele modelelor de regresie/clasificare, precum și construirea unor astfel de modele. În principal, tot ceea ce s-a construit, a avut ca punct de plecare setul de date **MNIST**, care conține 70,000 de imagini de dimensiune 28x28, reprezentând cifrele de la 0 la 9 scrise de mână. Pe baza acestor imagini, s-au antrenat modele de clasificare din ce în ce mai complexe, pornind de la **regresie logistică**, **KNN** și **SVM**, avansând la **rețele neuronale complet conectate (fully-connected)** și terminând cu **rețele neuronale de convoluție (CNN)**. Pentru fiecare model, setul de date a fost împărțit în date de antrenare (70%), date de validare în timpul antrenamentului (15%), precum și date de testare la finalul antrenamentului (15%). În plus, variabilele predictor (cei 784 de pixeli) au fost scalate între 0 și 1, folosind **scalarea (normalizarea) Min-Max**, ce face ca ele să \"cântărească\" la fel de mult și, în plus, crește abilitatea modelului de a învăța. Formula folosită pentru normalizare este: $X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}}$.\n",
    "\n",
    "Modelele au fost antrenate **stochastic**, datele fiind procesate în mini-batch-uri și, în plus, s-au folosit metode care vizează:\n",
    "+ înlăturarea **overfitting-ului** (\"învățare pe de rost\"): regularizare, dropout;\n",
    "+ optimizarea procesului de găsire a minimului funcției de cost: momentum, descreștere a coeficientului de învățare (learning rate decay)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script src=\"https://gist.github.com/jonschlinkert/5854601.js\"></script>\n",
    "# Conținut\n",
    "  1. [<font color='black'>Regresie logistică</font>](#chapter-1)\n",
    "  2. [<font color='black'>KNN</font>](#chapter-2)\n",
    "  3. [<font color='black'>SVM</font>](#chapter-3)\n",
    "  4. [<font color='black'>Rețele neuronale complet conectate</font>](#chapter-4)\n",
    "  5. [<font color='black'>Rețele neuronale de convoluție (CNN)</font>](#chapter-5)\n",
    "  6. [<font color='black'>Fereastră glisantă</font>](#chapter-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresie logistică <a id='chapter-1'></a>\n",
    "\n",
    "Fiind vorba de 784 variabile predictor (784 pixeli) și de 10 categorii, s-au folosit 10x785 de parametri (câte un regresor pentru fiecare categorie). Aceștia au fost inițializați cu valoarea 0. Hiperparametrii folosiți pentru antrenament sunt urmatorii: **număr de epoci**: 25; **coeficient de învățare**: 0.001; **dimensiunea unui mini-batch**: 10; **coeficient regularizare**: 0.0001; **momentum**: 0.9; **factor decay**: 0.65.\n",
    "\n",
    "În timpul antrenamentului au fost înregistrate la fiecare epocă loguri relevante pentru a observa modul în care converge modelul (costul per mini-batch, target real vs target prezis, cost + acuratețe pe seturile de date de antrenament și validare, timpul scurs pe ecopa respectivă):\n",
    "\n",
    "<img src=\"logreg_logs.png\" alt=\"Logs\" style=\"width: 450px;\"/>\n",
    " \n",
    "În urma antrenamentului modelului (care a durat aproximativ 60 de secunde), s-au obținut următoarele valori pentru acuratețe:\n",
    "+ pe setul de date folosit pentru antrenament: 92.45%\n",
    "+ pe setul de date de validare: 91.94%\n",
    "+ pe setul de date de test: 92.27%\n",
    "\n",
    "Convergența modelului se poate observa urmărind cele 2 grafice de mai jos:\n",
    "\n",
    "<img align=\"left\" src=\"logreg_cost_train.png\" alt=\"Cost antrenament\" style=\"width: 450px;\"/>\n",
    "<img align=\"center\" src=\"logreg_cost_val.png\" alt=\"Cost validare\" style=\"width: 450px;\"/>\n",
    "\n",
    "S-au folosit tehnici de afișare a celor 10 regresoare și s-a observat faptul că ele iau forma cifrei pe care doresc să o clasifice (adică parametrii vor avea valori foarte mari pozitive acolo unde pixelii sunt activați și valori foarte mari negative acolo unde pixelii sunt neactivați). Motivul apariției acestui fenomen este faptul că regresorul care încearcă să clasifice cifra $i, i = 0 ... 9$ trebuie să întoarcă o valoare cât mai mare pentru o imagine care are target $= i$ și o valoare cât mai mică pentru o imagine cu target $!= i$, astfel încât aplicarea funcției softmax pentru predicție să scaleze aceste valori corespunzător între 0 și 1 pentru oferirea unei probabilități. Mai mult, odată cu afișarea celor 10 vectori $\\theta$ ajungem la concluzia că **regresia logistică este o metodă de clasificare pixel-wise (nu învață trasături/caracteristici)**.\n",
    "\n",
    "<img src=\"thetas.png\" alt=\"Theta\"/>\n",
    "\n",
    "Pentru a observa în ce măsură s-a făcut predicția pe setul de date de test, s-au folosit procedee din domeniul statisticii precum: matricea de confuzie, precizie (cât din ce a fost prezis este și corect), recall (cât din ce este corect a fost prezis), scor F (media armonică a preciziei și a recall-ului):\n",
    "\n",
    "<img src=\"logreg_confuzie.png\" alt=\"Matrice confuzie\" style=\"width: 450px;\"/>\n",
    "\n",
    "| Precizie | Recall | F | N Obs\n",
    "--- | --- | --- | --- | ---\n",
    "0 | 0.95 | 0.97 | 0.96 | 1046\n",
    "1 | 0.96 | 0.97 | 0.97 | 1154\n",
    "2 | 0.94 | 0.88 | 0.91 | 1120\n",
    "3 | 0.90 | 0.92 | 0.91 | 1090\n",
    "4 | 0.91 | 0.93 | 0.92 | 1056\n",
    "5 | 0.89 | 0.85 | 0.87 | 965\n",
    "6 | 0.94 | 0.95 | 0.94 | 989\n",
    "7 | 0.93 | 0.94 | 0.94 | 1077\n",
    "8 | 0.89 | 0.91 | 0.90 | 985\n",
    "9 | 0.90 | 0.89 | 0.90 | 1018\n",
    "  |      |      |      |\n",
    "avg | 0.92 | 0.92 | 0.92 | 10500\n",
    "\n",
    "<img src=\"logreg_real_pred.png\" alt=\"Real vs pred\" style=\"width: 650px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN <a id='chapter-2'></a>\n",
    "\n",
    "**KNN** (K nearest neighbors) reprezintă unul din cei mai simpli algoritmi folosiți pentru clasificare/regresie, asta deoarece este *non-parametric*, astfel că datele de antrenare nu sunt folosite pentru a face o *generalizare*, ci pentru a face direct partea de predicție. \n",
    "\n",
    "Ce înseamnă acest lucru? Algoritmul se uită la o observație din setul de date de test și calculează distanța dintre aceasta și toate observațiile din setul de date de antrenament. La final se aleg cele mai mici k sume și, în cazul clasficării, se alege categoria ca fiind cea care are majoritate între cele k descoperite cu ajutorul algoritmului.\n",
    "\n",
    "Distanța dintre două imagini poate fi calculată în moduri diferite (dist. Euclidiană, Manhattan etc.), iar noi am folosit-o pe cea Manhattan, deoarece se comportă mai bine pentru date de dimensiuni mari (în cazul nostru dimensiunea este 784).\n",
    "\n",
    "Deși **KNN** se descurcă foarte bine la clasificarea imaginilor din setul de date MNIST, **acuratețea fiind de 96.44%**, costul computațional este unul foarte mare. Rularea algoritmului pe MNIST (80% train, 20% test) a durat aproximativ 1 oră si 30 de minute (pentru o abordare iterativă - fiecare intrare de test era tratată separat) și în jur de 11 minute (folosind puterea de calcul a bibliotecii **sklearn**: <a href='http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html'>pairwise distances</a> - ce calculează toate distanțele vectorizat). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM <a id='chapter-3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Rețele neuronale complet conectate <a id='chapter-4'></a>\n",
    "\n",
    "Problema regresiei liniare constă în faptul că nu surprinde deloc non-liniaritățile. Tot ce reușește să facă este să găsească o linie (hiperplan pentru $n > 2$) care să aproximeze cât mai bine punctele (observațiile). Pentru a scăpa de acest inconvenient, am creat o rețea neuronală complet conectată formată dintr-un nivel de intrare (cu 784 neuroni), un nivel ascuns (cu 256 neuroni) și un nivel de ieșire (cu 10 neuroni), care folosește o funcție non-liniară (**ReLU**: $relu(z) = max(0,z)$ **sigmoid**) pentru a produce output-ul fiecărui nivel ascuns și, evident, **softmax** la nivelul de ieșire.\n",
    "\n",
    "<img src=\"neural_network.png\" alt=\"Retea neurala\" style=\"width: 600px;\"/>\n",
    "\n",
    "1. <h6>Forward propagation</h6>\n",
    "\n",
    "    Pentru fiecare nivel, matricea de parametri care trebuie sa fie tunați va fi de dimensiune $(nrUnitsPrev + 1) \\times nrUnitsCurrent$ (+1 pentru **bias**). Astfel, în cazul arhitecturii 784-256-10, au fost folosiți 203,530 parametri, ocupând 0.78MB din memoria totală a calculatorului.\n",
    "\n",
    "    Intrarea pentru primul nivel este chiar setul de date. Acesta este totodată și ieșire. Întotdeauna ieșirea nivelului $i-1$ devine intrare pentru nivelul $i, i = 1 .. n-1$. Astfel, pentru nivelul ascuns se calculează produsul dintre intrare și matricea de parametri - obținând matricea $z$. Intervine însă introducerea non-liniarității care, care va genera ieșirea nivelului (matricea $a$). Asemănător, se calculează matricea $z$ și pentru nivelul de ieșire, după care se aplică **softmax** pentru a genera cele 10 probabilități.\n",
    "    \n",
    "    $z^{(1)} = XW_0 \\\\\n",
    "    a^{(1)} = f(z^{(1)}) \\\\\n",
    "    z^{(2)} = a^{(1)}W_1 \\\\\n",
    "    a^{(2)} = f(z^{(2)}) = \\hat{y}$\n",
    "    \n",
    "2. <h6>Backward propagation</h6>\n",
    "\n",
    "    Tunarea parametrilor se face tot cu ajutorul metodei **gradientului descendent**.\n",
    "    \n",
    "    $\\frac{\\partial J}{\\partial W_1} = \\frac{1}{m}input^T\\cdot\\delta_2 =  \\frac{1}{m}a^{(1)T}\\cdot(\\hat{y} - y) \\\\ \\\\\n",
    "    \\frac{\\partial J}{\\partial W_0} = \\frac{1}{m}input^T\\cdot\\delta_1 = \\frac{1}{m}X^T\\cdot[(\\delta_2\\cdot W_1^T) \\ast fAct']\\\\\n",
    "    W_1 := W_1 - \\frac{\\alpha}{m}\\frac{\\partial J}{\\partial W_1}\\\\\n",
    "    W_0 := W_0 - \\frac{\\alpha}{m}\\frac{\\partial J}{\\partial W_0}\n",
    "    $\n",
    "    \n",
    "    Funcția de activare folosită este **ReLU**, a cărei derivată arată astfel:\n",
    "    \n",
    "    $f'(z)=\\begin{cases} \n",
    "      1 & dacă\\;z > 0 \\\\\n",
    "      0 & altfel\n",
    "   \\end{cases}$\n",
    "   \n",
    "   În contrast, graficul derivatei funcției **sigmoid** arată faptul că pe masură ce valoarea sigmoidată este din ce în ce mai mare, derivata în acel punct devine din ce în ce mai mică, ceea ce înseamnă că gradientul va deveni nesemnificativ și astfel, rețeaua nu va fi capabilă să învețe foarte bine:\n",
    "   \n",
    "   <img src=\"derivative_sigmoid.png\" alt=\"Sigmoid\" style=\"width: 350px;\"/>\n",
    "   <br>\n",
    "   \n",
    "   \n",
    "3. <h6>Antrenarea modelului</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rețele neuronale de convoluție (CNN) <a id='chapter-5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fereastră glisantă <a id='chapter-6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
