[2017.09.05-11:09:43] Fetch MNIST Data Set
[2017.09.05-11:09:44] Finished fetching MNIST Data Set
[2017.09.05-11:09:44] Initialize data preprocessor
[2017.09.05-11:09:44] Start preprocessing data
[2017.09.05-11:09:44] Normalize data
[2017.09.05-11:09:45] Finished normalizing data
[2017.09.05-11:09:45] Split in train set and test set by 14.000000000000002
[2017.09.05-11:09:45] Finished splitting data
[2017.09.05-11:09:45] Initialize simple gradient descendent logisitic regression solver
[2017.09.05-11:09:45] Computing theta for target = 0
[2017.09.05-11:09:45] Start simple without reg training: alpha=0.01, batchSz=10, beta=0
[2017.09.05-11:09:46] eph#1, cost decreased by 0.0020 ==> increasing alpha to 0.0105
[2017.09.05-11:09:46] eph#2, cost decreased by 0.0009 ==> increasing alpha to 0.0110
[2017.09.05-11:09:47] eph#3, cost decreased by 0.0005 ==> increasing alpha to 0.0116
[2017.09.05-11:09:47] cost_eph# 3 = 0.0059; abs diff between current and last eph = 0.0005
[2017.09.05-11:09:47] eph# 3, gradient[380:385] = [  5.127481e-04   5.515747e-05   2.289789e-05   6.997275e-05  -7.513948e-05]
[2017.09.05-11:09:47] eph#4, cost decreased by 0.0004 ==> increasing alpha to 0.0122
[2017.09.05-11:09:47] eph#5, cost decreased by 0.0003 ==> increasing alpha to 0.0128
[2017.09.05-11:09:48] eph#6, cost decreased by 0.0002 ==> increasing alpha to 0.0134
[2017.09.05-11:09:48] cost_eph# 6 = 0.0050; abs diff between current and last eph = 0.0002
[2017.09.05-11:09:48] eph# 6, gradient[380:385] = [  3.486774e-04   3.065220e-05   1.432152e-05   3.369497e-05  -3.524249e-05]
[2017.09.05-11:09:48] eph#7, cost decreased by 0.0002 ==> increasing alpha to 0.0141
[2017.09.05-11:09:49] eph#8, cost decreased by 0.0002 ==> increasing alpha to 0.0148
[2017.09.05-11:09:49] eph#9, cost decreased by 0.0001 ==> increasing alpha to 0.0155
[2017.09.05-11:09:49] cost_eph# 9 = 0.0046; abs diff between current and last eph = 0.0001
[2017.09.05-11:09:49] eph# 9, gradient[380:385] = [  2.613689e-04   2.076472e-05   1.014052e-05   2.045298e-05  -2.401938e-05]
[2017.09.05-11:09:49] eph#10, cost decreased by 0.0001 ==> increasing alpha to 0.0163
[2017.09.05-11:09:50] eph#11, cost decreased by 0.0001 ==> increasing alpha to 0.0171
[2017.09.05-11:09:50] eph#12, cost decreased by 0.0001 ==> increasing alpha to 0.0180
[2017.09.05-11:09:50] eph#12, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:09:50] Time for simple without reg training = 5.136s
[2017.09.05-11:09:50] Computing theta for target = 1
[2017.09.05-11:09:50] Start simple without reg training: alpha=0.01, batchSz=10, beta=0
[2017.09.05-11:09:51] eph#1, cost decreased by 0.0018 ==> increasing alpha to 0.0105
[2017.09.05-11:09:51] eph#2, cost decreased by 0.0008 ==> increasing alpha to 0.0110
[2017.09.05-11:09:52] eph#3, cost decreased by 0.0005 ==> increasing alpha to 0.0116
[2017.09.05-11:09:52] cost_eph# 3 = 0.0058; abs diff between current and last eph = 0.0005
[2017.09.05-11:09:52] eph# 3, gradient[380:385] = [  3.531432e-03  -9.426236e-04   2.009642e-05   2.323680e-05   1.880532e-05]
[2017.09.05-11:09:52] eph#4, cost decreased by 0.0003 ==> increasing alpha to 0.0122
[2017.09.05-11:09:53] eph#5, cost decreased by 0.0002 ==> increasing alpha to 0.0128
[2017.09.05-11:09:53] eph#6, cost decreased by 0.0002 ==> increasing alpha to 0.0134
[2017.09.05-11:09:53] cost_eph# 6 = 0.0050; abs diff between current and last eph = 0.0002
[2017.09.05-11:09:53] eph# 6, gradient[380:385] = [  2.997976e-03  -7.377432e-04   6.256870e-06   6.889115e-06   5.484582e-06]
[2017.09.05-11:09:53] eph#7, cost decreased by 0.0002 ==> increasing alpha to 0.0141
[2017.09.05-11:09:54] eph#8, cost decreased by 0.0001 ==> increasing alpha to 0.0148
[2017.09.05-11:09:54] eph#9, cost decreased by 0.0001 ==> increasing alpha to 0.0155
[2017.09.05-11:09:54] cost_eph# 9 = 0.0046; abs diff between current and last eph = 0.0001
[2017.09.05-11:09:54] eph# 9, gradient[380:385] = [  2.470679e-03  -6.666956e-04   2.772331e-06   2.963282e-06   2.338794e-06]
[2017.09.05-11:09:55] eph#10, cost decreased by 0.0001 ==> increasing alpha to 0.0163
[2017.09.05-11:09:55] eph#11, cost decreased by 0.0001 ==> increasing alpha to 0.0171
[2017.09.05-11:09:55] eph#11, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:09:55] Time for simple without reg training = 4.670s
[2017.09.05-11:09:55] Computing theta for target = 2
[2017.09.05-11:09:55] Start simple without reg training: alpha=0.01, batchSz=10, beta=0
[2017.09.05-11:09:56] eph#1, cost decreased by 0.0031 ==> increasing alpha to 0.0105
[2017.09.05-11:09:56] eph#2, cost decreased by 0.0014 ==> increasing alpha to 0.0110
[2017.09.05-11:09:56] eph#3, cost decreased by 0.0008 ==> increasing alpha to 0.0116
[2017.09.05-11:09:56] cost_eph# 3 = 0.0121; abs diff between current and last eph = 0.0008
[2017.09.05-11:09:56] eph# 3, gradient[380:385] = [-0.008509  0.002763  0.007395  0.010127  0.010414]
[2017.09.05-11:09:57] eph#4, cost decreased by 0.0006 ==> increasing alpha to 0.0122
[2017.09.05-11:09:57] eph#5, cost decreased by 0.0004 ==> increasing alpha to 0.0128
[2017.09.05-11:09:58] eph#6, cost decreased by 0.0004 ==> increasing alpha to 0.0134
[2017.09.05-11:09:58] cost_eph# 6 = 0.0108; abs diff between current and last eph = 0.0004
[2017.09.05-11:09:58] eph# 6, gradient[380:385] = [-0.009166  0.001777  0.005518  0.007935  0.008008]
[2017.09.05-11:09:58] eph#7, cost decreased by 0.0003 ==> increasing alpha to 0.0141
[2017.09.05-11:09:58] eph#8, cost decreased by 0.0002 ==> increasing alpha to 0.0148
[2017.09.05-11:09:59] eph#9, cost decreased by 0.0002 ==> increasing alpha to 0.0155
[2017.09.05-11:09:59] cost_eph# 9 = 0.0100; abs diff between current and last eph = 0.0002
[2017.09.05-11:09:59] eph# 9, gradient[380:385] = [-0.009572  0.001169  0.004013  0.006162  0.006186]
[2017.09.05-11:09:59] eph#10, cost decreased by 0.0002 ==> increasing alpha to 0.0163
[2017.09.05-11:10:00] eph#11, cost decreased by 0.0002 ==> increasing alpha to 0.0171
[2017.09.05-11:10:00] eph#12, cost decreased by 0.0001 ==> increasing alpha to 0.0180
[2017.09.05-11:10:00] cost_eph#12 = 0.0095; abs diff between current and last eph = 0.0001
[2017.09.05-11:10:00] eph#12, gradient[380:385] = [-0.009866  0.000753  0.002924  0.004836  0.00484 ]
[2017.09.05-11:10:00] eph#13, cost decreased by 0.0001 ==> increasing alpha to 0.0189
[2017.09.05-11:10:01] eph#14, cost decreased by 0.0001 ==> increasing alpha to 0.0198
[2017.09.05-11:10:01] Time for simple without reg training = 5.804s
[2017.09.05-11:10:01] Computing theta for target = 3
[2017.09.05-11:10:01] Start simple without reg training: alpha=0.01, batchSz=10, beta=0
[2017.09.05-11:10:02] eph#1, cost decreased by 0.0029 ==> increasing alpha to 0.0105
[2017.09.05-11:10:02] eph#2, cost decreased by 0.0013 ==> increasing alpha to 0.0110
[2017.09.05-11:10:02] eph#3, cost decreased by 0.0008 ==> increasing alpha to 0.0116
[2017.09.05-11:10:02] cost_eph# 3 = 0.0140; abs diff between current and last eph = 0.0008
[2017.09.05-11:10:02] eph# 3, gradient[380:385] = [ 0.005064  0.002035  0.007583  0.007895  0.004374]
[2017.09.05-11:10:03] eph#4, cost decreased by 0.0006 ==> increasing alpha to 0.0122
[2017.09.05-11:10:03] eph#5, cost decreased by 0.0004 ==> increasing alpha to 0.0128
[2017.09.05-11:10:03] eph#6, cost decreased by 0.0004 ==> increasing alpha to 0.0134
[2017.09.05-11:10:03] cost_eph# 6 = 0.0126; abs diff between current and last eph = 0.0004
[2017.09.05-11:10:03] eph# 6, gradient[380:385] = [ 0.00369   0.001414  0.005348  0.005578  0.002902]
[2017.09.05-11:10:04] eph#7, cost decreased by 0.0003 ==> increasing alpha to 0.0141
[2017.09.05-11:10:04] eph#8, cost decreased by 0.0003 ==> increasing alpha to 0.0148
[2017.09.05-11:10:05] eph#9, cost decreased by 0.0002 ==> increasing alpha to 0.0155
[2017.09.05-11:10:05] cost_eph# 9 = 0.0118; abs diff between current and last eph = 0.0002
[2017.09.05-11:10:05] eph# 9, gradient[380:385] = [ 0.002852  0.001025  0.003771  0.003947  0.001874]
[2017.09.05-11:10:05] eph#10, cost decreased by 0.0002 ==> increasing alpha to 0.0163
[2017.09.05-11:10:05] eph#11, cost decreased by 0.0002 ==> increasing alpha to 0.0171
[2017.09.05-11:10:06] eph#12, cost decreased by 0.0002 ==> increasing alpha to 0.0180
[2017.09.05-11:10:06] cost_eph#12 = 0.0112; abs diff between current and last eph = 0.0002
[2017.09.05-11:10:06] eph#12, gradient[380:385] = [ 0.002276  0.000772  0.002732  0.002872  0.001219]
[2017.09.05-11:10:06] eph#13, cost decreased by 0.0002 ==> increasing alpha to 0.0189
[2017.09.05-11:10:07] eph#14, cost decreased by 0.0001 ==> increasing alpha to 0.0198
[2017.09.05-11:10:07] Time for simple without reg training = 5.786s
[2017.09.05-11:10:07] Computing theta for target = 4
[2017.09.05-11:10:07] Start simple without reg training: alpha=0.01, batchSz=10, beta=0
[2017.09.05-11:10:07] eph#1, cost decreased by 0.0031 ==> increasing alpha to 0.0105
[2017.09.05-11:10:08] eph#2, cost decreased by 0.0014 ==> increasing alpha to 0.0110
[2017.09.05-11:10:08] eph#3, cost decreased by 0.0008 ==> increasing alpha to 0.0116
[2017.09.05-11:10:08] cost_eph# 3 = 0.0110; abs diff between current and last eph = 0.0008
[2017.09.05-11:10:08] eph# 3, gradient[380:385] = [ 0.005803  0.009506  0.009497  0.011194  0.009024]
[2017.09.05-11:10:08] eph#4, cost decreased by 0.0006 ==> increasing alpha to 0.0122
[2017.09.05-11:10:09] eph#5, cost decreased by 0.0004 ==> increasing alpha to 0.0128
[2017.09.05-11:10:09] eph#6, cost decreased by 0.0003 ==> increasing alpha to 0.0134
[2017.09.05-11:10:09] cost_eph# 6 = 0.0096; abs diff between current and last eph = 0.0003
[2017.09.05-11:10:09] eph# 6, gradient[380:385] = [ 0.006855  0.011426  0.011503  0.012732  0.010119]
[2017.09.05-11:10:10] eph#7, cost decreased by 0.0003 ==> increasing alpha to 0.0141
[2017.09.05-11:10:10] eph#8, cost decreased by 0.0002 ==> increasing alpha to 0.0148
[2017.09.05-11:10:10] eph#9, cost decreased by 0.0002 ==> increasing alpha to 0.0155
[2017.09.05-11:10:10] cost_eph# 9 = 0.0089; abs diff between current and last eph = 0.0002
[2017.09.05-11:10:10] eph# 9, gradient[380:385] = [ 0.007536  0.012656  0.012794  0.013784  0.010885]
[2017.09.05-11:10:11] eph#10, cost decreased by 0.0002 ==> increasing alpha to 0.0163
[2017.09.05-11:10:11] eph#11, cost decreased by 0.0002 ==> increasing alpha to 0.0171
[2017.09.05-11:10:12] eph#12, cost decreased by 0.0001 ==> increasing alpha to 0.0180
[2017.09.05-11:10:12] cost_eph#12 = 0.0085; abs diff between current and last eph = 0.0001
[2017.09.05-11:10:12] eph#12, gradient[380:385] = [ 0.007972  0.013446  0.013625  0.014447  0.011366]
[2017.09.05-11:10:12] eph#13, cost decreased by 0.0001 ==> increasing alpha to 0.0189
[2017.09.05-11:10:12] eph#14, cost decreased by 0.0001 ==> increasing alpha to 0.0198
[2017.09.05-11:10:12] Time for simple without reg training = 5.819s
[2017.09.05-11:10:12] Computing theta for target = 5
[2017.09.05-11:10:12] Start simple without reg training: alpha=0.01, batchSz=10, beta=0
[2017.09.05-11:10:13] eph#1, cost decreased by 0.0039 ==> increasing alpha to 0.0105
[2017.09.05-11:10:13] eph#2, cost decreased by 0.0018 ==> increasing alpha to 0.0110
[2017.09.05-11:10:14] eph#3, cost decreased by 0.0011 ==> increasing alpha to 0.0116
[2017.09.05-11:10:14] cost_eph# 3 = 0.0164; abs diff between current and last eph = 0.0011
[2017.09.05-11:10:14] eph# 3, gradient[380:385] = [  5.842178e-04   1.819442e-04   1.999897e-05   2.241177e-04   3.228081e-04]
[2017.09.05-11:10:14] eph#4, cost decreased by 0.0008 ==> increasing alpha to 0.0122
[2017.09.05-11:10:15] eph#5, cost decreased by 0.0006 ==> increasing alpha to 0.0128
[2017.09.05-11:10:15] eph#6, cost decreased by 0.0005 ==> increasing alpha to 0.0134
[2017.09.05-11:10:15] cost_eph# 6 = 0.0146; abs diff between current and last eph = 0.0005
[2017.09.05-11:10:15] eph# 6, gradient[380:385] = [  3.158348e-04   8.952753e-05   5.924076e-06   9.929025e-05   1.602014e-04]
[2017.09.05-11:10:15] eph#7, cost decreased by 0.0004 ==> increasing alpha to 0.0141
[2017.09.05-11:10:16] eph#8, cost decreased by 0.0003 ==> increasing alpha to 0.0148
[2017.09.05-11:10:16] eph#9, cost decreased by 0.0003 ==> increasing alpha to 0.0155
[2017.09.05-11:10:16] cost_eph# 9 = 0.0136; abs diff between current and last eph = 0.0003
[2017.09.05-11:10:16] eph# 9, gradient[380:385] = [  2.042930e-04   5.518315e-05   2.578361e-06   5.408106e-05   9.750423e-05]
[2017.09.05-11:10:17] eph#10, cost decreased by 0.0003 ==> increasing alpha to 0.0163
[2017.09.05-11:10:17] eph#11, cost decreased by 0.0002 ==> increasing alpha to 0.0171
[2017.09.05-11:10:17] eph#12, cost decreased by 0.0002 ==> increasing alpha to 0.0180
[2017.09.05-11:10:17] cost_eph#12 = 0.0129; abs diff between current and last eph = 0.0002
[2017.09.05-11:10:17] eph#12, gradient[380:385] = [  1.460673e-04   3.781698e-05   1.373209e-06   3.326957e-05   6.646873e-05]
[2017.09.05-11:10:18] eph#13, cost decreased by 0.0002 ==> increasing alpha to 0.0189
[2017.09.05-11:10:18] eph#14, cost decreased by 0.0002 ==> increasing alpha to 0.0198
[2017.09.05-11:10:18] Time for simple without reg training = 5.807s
[2017.09.05-11:10:18] Computing theta for target = 6
[2017.09.05-11:10:18] Start simple without reg training: alpha=0.01, batchSz=10, beta=0
[2017.09.05-11:10:19] eph#1, cost decreased by 0.0024 ==> increasing alpha to 0.0105
[2017.09.05-11:10:19] eph#2, cost decreased by 0.0010 ==> increasing alpha to 0.0110
[2017.09.05-11:10:20] eph#3, cost decreased by 0.0006 ==> increasing alpha to 0.0116
[2017.09.05-11:10:20] cost_eph# 3 = 0.0079; abs diff between current and last eph = 0.0006
[2017.09.05-11:10:20] eph# 3, gradient[380:385] = [-0.001753 -0.003092 -0.003092 -0.002528 -0.001948]
[2017.09.05-11:10:20] eph#4, cost decreased by 0.0004 ==> increasing alpha to 0.0122
[2017.09.05-11:10:20] eph#5, cost decreased by 0.0003 ==> increasing alpha to 0.0128
[2017.09.05-11:10:21] eph#6, cost decreased by 0.0002 ==> increasing alpha to 0.0134
[2017.09.05-11:10:21] cost_eph# 6 = 0.0069; abs diff between current and last eph = 0.0002
[2017.09.05-11:10:21] eph# 6, gradient[380:385] = [-0.001437 -0.002507 -0.00255  -0.002263 -0.001733]
[2017.09.05-11:10:21] eph#7, cost decreased by 0.0002 ==> increasing alpha to 0.0141
[2017.09.05-11:10:22] eph#8, cost decreased by 0.0002 ==> increasing alpha to 0.0148
[2017.09.05-11:10:22] eph#9, cost decreased by 0.0001 ==> increasing alpha to 0.0155
[2017.09.05-11:10:22] cost_eph# 9 = 0.0064; abs diff between current and last eph = 0.0001
[2017.09.05-11:10:22] eph# 9, gradient[380:385] = [-0.001228 -0.00213  -0.002177 -0.001996 -0.001528]
[2017.09.05-11:10:22] eph#10, cost decreased by 0.0001 ==> increasing alpha to 0.0163
[2017.09.05-11:10:23] eph#11, cost decreased by 0.0001 ==> increasing alpha to 0.0171
[2017.09.05-11:10:23] eph#12, cost decreased by 0.0001 ==> increasing alpha to 0.0180
[2017.09.05-11:10:23] eph#12, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:10:23] Time for simple without reg training = 5.022s
[2017.09.05-11:10:23] Computing theta for target = 7
[2017.09.05-11:10:23] Start simple without reg training: alpha=0.01, batchSz=10, beta=0
[2017.09.05-11:10:24] eph#1, cost decreased by 0.0021 ==> increasing alpha to 0.0105
[2017.09.05-11:10:24] eph#2, cost decreased by 0.0009 ==> increasing alpha to 0.0110
[2017.09.05-11:10:25] eph#3, cost decreased by 0.0006 ==> increasing alpha to 0.0116
[2017.09.05-11:10:25] cost_eph# 3 = 0.0088; abs diff between current and last eph = 0.0006
[2017.09.05-11:10:25] eph# 3, gradient[380:385] = [ 0.002604  0.001065 -0.003398 -0.000801 -0.000909]
[2017.09.05-11:10:25] eph#4, cost decreased by 0.0004 ==> increasing alpha to 0.0122
[2017.09.05-11:10:25] eph#5, cost decreased by 0.0003 ==> increasing alpha to 0.0128
[2017.09.05-11:10:26] eph#6, cost decreased by 0.0002 ==> increasing alpha to 0.0134
[2017.09.05-11:10:26] cost_eph# 6 = 0.0079; abs diff between current and last eph = 0.0002
[2017.09.05-11:10:26] eph# 6, gradient[380:385] = [ 0.002692  0.001317 -0.002417 -0.0005   -0.000573]
[2017.09.05-11:10:26] eph#7, cost decreased by 0.0002 ==> increasing alpha to 0.0141
[2017.09.05-11:10:27] eph#8, cost decreased by 0.0002 ==> increasing alpha to 0.0148
[2017.09.05-11:10:27] eph#9, cost decreased by 0.0001 ==> increasing alpha to 0.0155
[2017.09.05-11:10:27] cost_eph# 9 = 0.0074; abs diff between current and last eph = 0.0001
[2017.09.05-11:10:27] eph# 9, gradient[380:385] = [ 0.002624  0.001358 -0.001981 -0.000478 -0.000535]
[2017.09.05-11:10:27] eph#10, cost decreased by 0.0001 ==> increasing alpha to 0.0163
[2017.09.05-11:10:28] eph#11, cost decreased by 0.0001 ==> increasing alpha to 0.0171
[2017.09.05-11:10:28] eph#12, cost decreased by 0.0001 ==> increasing alpha to 0.0180
[2017.09.05-11:10:28] eph#12, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:10:28] Time for simple without reg training = 5.043s
[2017.09.05-11:10:28] Computing theta for target = 8
[2017.09.05-11:10:28] Start simple without reg training: alpha=0.01, batchSz=10, beta=0
[2017.09.05-11:10:29] eph#1, cost decreased by 0.0044 ==> increasing alpha to 0.0105
[2017.09.05-11:10:29] eph#2, cost decreased by 0.0020 ==> increasing alpha to 0.0110
[2017.09.05-11:10:30] eph#3, cost decreased by 0.0012 ==> increasing alpha to 0.0116
[2017.09.05-11:10:30] cost_eph# 3 = 0.0226; abs diff between current and last eph = 0.0012
[2017.09.05-11:10:30] eph# 3, gradient[380:385] = [ 0.006833  0.001349  0.0007    0.00112   0.001027]
[2017.09.05-11:10:30] eph#4, cost decreased by 0.0009 ==> increasing alpha to 0.0122
[2017.09.05-11:10:31] eph#5, cost decreased by 0.0007 ==> increasing alpha to 0.0128
[2017.09.05-11:10:31] eph#6, cost decreased by 0.0006 ==> increasing alpha to 0.0134
[2017.09.05-11:10:31] cost_eph# 6 = 0.0205; abs diff between current and last eph = 0.0006
[2017.09.05-11:10:31] eph# 6, gradient[380:385] = [ 0.005719  0.000913  0.000426  0.000656  0.000605]
[2017.09.05-11:10:31] eph#7, cost decreased by 0.0005 ==> increasing alpha to 0.0141
[2017.09.05-11:10:32] eph#8, cost decreased by 0.0004 ==> increasing alpha to 0.0148
[2017.09.05-11:10:32] eph#9, cost decreased by 0.0004 ==> increasing alpha to 0.0155
[2017.09.05-11:10:32] cost_eph# 9 = 0.0192; abs diff between current and last eph = 0.0004
[2017.09.05-11:10:32] eph# 9, gradient[380:385] = [ 0.004847  0.000717  0.000322  0.000462  0.000426]
[2017.09.05-11:10:32] eph#10, cost decreased by 0.0003 ==> increasing alpha to 0.0163
[2017.09.05-11:10:33] eph#11, cost decreased by 0.0003 ==> increasing alpha to 0.0171
[2017.09.05-11:10:33] eph#12, cost decreased by 0.0003 ==> increasing alpha to 0.0180
[2017.09.05-11:10:33] cost_eph#12 = 0.0183; abs diff between current and last eph = 0.0003
[2017.09.05-11:10:33] eph#12, gradient[380:385] = [ 0.004057  0.000589  0.000267  0.000355  0.000326]
[2017.09.05-11:10:34] eph#13, cost decreased by 0.0003 ==> increasing alpha to 0.0189
[2017.09.05-11:10:34] eph#14, cost decreased by 0.0002 ==> increasing alpha to 0.0198
[2017.09.05-11:10:34] Time for simple without reg training = 5.816s
[2017.09.05-11:10:34] Computing theta for target = 9
[2017.09.05-11:10:34] Start simple without reg training: alpha=0.01, batchSz=10, beta=0
[2017.09.05-11:10:35] eph#1, cost decreased by 0.0040 ==> increasing alpha to 0.0105
[2017.09.05-11:10:35] eph#2, cost decreased by 0.0019 ==> increasing alpha to 0.0110
[2017.09.05-11:10:36] eph#3, cost decreased by 0.0011 ==> increasing alpha to 0.0116
[2017.09.05-11:10:36] cost_eph# 3 = 0.0188; abs diff between current and last eph = 0.0011
[2017.09.05-11:10:36] eph# 3, gradient[380:385] = [ 0.001722  0.003117  0.004532 -0.003889 -0.004214]
[2017.09.05-11:10:36] eph#4, cost decreased by 0.0008 ==> increasing alpha to 0.0122
[2017.09.05-11:10:36] eph#5, cost decreased by 0.0006 ==> increasing alpha to 0.0128
[2017.09.05-11:10:37] eph#6, cost decreased by 0.0005 ==> increasing alpha to 0.0134
[2017.09.05-11:10:37] cost_eph# 6 = 0.0169; abs diff between current and last eph = 0.0005
[2017.09.05-11:10:37] eph# 6, gradient[380:385] = [ 0.001755  0.003183  0.004271 -0.002568 -0.003001]
[2017.09.05-11:10:37] eph#7, cost decreased by 0.0004 ==> increasing alpha to 0.0141
[2017.09.05-11:10:38] eph#8, cost decreased by 0.0003 ==> increasing alpha to 0.0148
[2017.09.05-11:10:38] eph#9, cost decreased by 0.0003 ==> increasing alpha to 0.0155
[2017.09.05-11:10:38] cost_eph# 9 = 0.0159; abs diff between current and last eph = 0.0003
[2017.09.05-11:10:38] eph# 9, gradient[380:385] = [ 0.0018    0.003228  0.004063 -0.001983 -0.002474]
[2017.09.05-11:10:38] eph#10, cost decreased by 0.0003 ==> increasing alpha to 0.0163
[2017.09.05-11:10:39] eph#11, cost decreased by 0.0002 ==> increasing alpha to 0.0171
[2017.09.05-11:10:39] eph#12, cost decreased by 0.0002 ==> increasing alpha to 0.0180
[2017.09.05-11:10:39] cost_eph#12 = 0.0152; abs diff between current and last eph = 0.0002
[2017.09.05-11:10:39] eph#12, gradient[380:385] = [ 0.001824  0.003233  0.003872 -0.001761 -0.002281]
[2017.09.05-11:10:39] eph#13, cost decreased by 0.0002 ==> increasing alpha to 0.0189
[2017.09.05-11:10:40] eph#14, cost decreased by 0.0002 ==> increasing alpha to 0.0198
[2017.09.05-11:10:40] Time for simple without reg training = 5.811s
[2017.09.05-11:10:40] Total train time for simple without reg 54.714s
[2017.09.05-11:10:40] Results using simple without reg solver -- test
[2017.09.05-11:10:40] General accuracy results are: correct=8890, wrong=911, accuracy=90.71%
[2017.09.05-11:10:40] Printing results for target 0: correct=930, wrong=28, accuracy=97.08%
[2017.09.05-11:10:40] Printing results for target 1: correct=1072, wrong=28, accuracy=97.45%
[2017.09.05-11:10:40] Printing results for target 2: correct=867, wrong=115, accuracy=88.29%
[2017.09.05-11:10:40] Printing results for target 3: correct=874, wrong=114, accuracy=88.46%
[2017.09.05-11:10:40] Printing results for target 4: correct=842, wrong=65, accuracy=92.83%
[2017.09.05-11:10:40] Printing results for target 5: correct=756, wrong=147, accuracy=83.72%
[2017.09.05-11:10:40] Printing results for target 6: correct=974, wrong=37, accuracy=96.34%
[2017.09.05-11:10:40] Printing results for target 7: correct=960, wrong=93, accuracy=91.17%
[2017.09.05-11:10:40] Printing results for target 8: correct=829, wrong=134, accuracy=86.09%
[2017.09.05-11:10:40] Printing results for target 9: correct=786, wrong=150, accuracy=83.97%
[2017.09.05-11:10:40] Best accuracy is 97.45% for digit 1
[2017.09.05-11:10:40] Worst accuracy is 83.72% for digit 5
[2017.09.05-11:10:45] Results using simple without reg solver -- train
[2017.09.05-11:10:45] General accuracy results are: correct=54420, wrong=5779, accuracy=90.40%
[2017.09.05-11:10:45] Printing results for target 0: correct=5780, wrong=165, accuracy=97.22%
[2017.09.05-11:10:45] Printing results for target 1: correct=6554, wrong=223, accuracy=96.71%
[2017.09.05-11:10:45] Printing results for target 2: correct=5247, wrong=761, accuracy=87.33%
[2017.09.05-11:10:45] Printing results for target 3: correct=5406, wrong=747, accuracy=87.86%
[2017.09.05-11:10:45] Printing results for target 4: correct=5431, wrong=486, accuracy=91.79%
[2017.09.05-11:10:45] Printing results for target 5: correct=4444, wrong=966, accuracy=82.14%
[2017.09.05-11:10:45] Printing results for target 6: correct=5576, wrong=289, accuracy=95.07%
[2017.09.05-11:10:45] Printing results for target 7: correct=5724, wrong=516, accuracy=91.73%
[2017.09.05-11:10:45] Printing results for target 8: correct=4992, wrong=870, accuracy=85.16%
[2017.09.05-11:10:45] Printing results for target 9: correct=5266, wrong=756, accuracy=87.45%
[2017.09.05-11:10:45] Best accuracy is 97.22% for digit 0
[2017.09.05-11:10:45] Worst accuracy is 82.14% for digit 5
[2017.09.05-11:10:57] Initialize simple gradient descendent logisitic regression solver
[2017.09.05-11:10:57] Computing theta for target = 0
[2017.09.05-11:10:57] Start simple with reg training: alpha=0.01, batchSz=10, beta=0.001
[2017.09.05-11:10:58] eph#1, cost decreased by 0.0020 ==> increasing alpha to 0.0105
[2017.09.05-11:10:58] eph#2, cost decreased by 0.0009 ==> increasing alpha to 0.0110
[2017.09.05-11:10:58] eph#3, cost decreased by 0.0005 ==> increasing alpha to 0.0116
[2017.09.05-11:10:58] cost_eph# 3 = 0.0060; abs diff between current and last eph = 0.0005
[2017.09.05-11:10:58] eph# 3, gradient[380:385] = [  5.036876e-04   2.793439e-05  -3.197309e-07   6.185704e-05  -8.410928e-05]
[2017.09.05-11:10:59] eph#4, cost decreased by 0.0004 ==> increasing alpha to 0.0122
[2017.09.05-11:10:59] eph#5, cost decreased by 0.0003 ==> increasing alpha to 0.0128
[2017.09.05-11:10:59] eph#6, cost decreased by 0.0002 ==> increasing alpha to 0.0134
[2017.09.05-11:10:59] cost_eph# 6 = 0.0051; abs diff between current and last eph = 0.0002
[2017.09.05-11:10:59] eph# 6, gradient[380:385] = [  3.418969e-04  -1.299090e-06  -9.364989e-06   2.741354e-05  -4.433642e-05]
[2017.09.05-11:11:00] eph#7, cost decreased by 0.0002 ==> increasing alpha to 0.0141
[2017.09.05-11:11:00] eph#8, cost decreased by 0.0001 ==> increasing alpha to 0.0148
[2017.09.05-11:11:01] eph#9, cost decreased by 0.0001 ==> increasing alpha to 0.0155
[2017.09.05-11:11:01] cost_eph# 9 = 0.0046; abs diff between current and last eph = 0.0001
[2017.09.05-11:11:01] eph# 9, gradient[380:385] = [  2.566701e-04  -1.443627e-05  -1.301501e-05   1.588680e-05  -3.387079e-05]
[2017.09.05-11:11:01] eph#10, cost decreased by 0.0001 ==> increasing alpha to 0.0163
[2017.09.05-11:11:01] eph#11, cost decreased by 0.0001 ==> increasing alpha to 0.0171
[2017.09.05-11:11:01] eph#11, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:11:01] Time for simple with reg training = 4.719s
[2017.09.05-11:11:01] Computing theta for target = 1
[2017.09.05-11:11:01] Start simple with reg training: alpha=0.01, batchSz=10, beta=0.001
[2017.09.05-11:11:02] eph#1, cost decreased by 0.0018 ==> increasing alpha to 0.0105
[2017.09.05-11:11:03] eph#2, cost decreased by 0.0008 ==> increasing alpha to 0.0110
[2017.09.05-11:11:03] eph#3, cost decreased by 0.0005 ==> increasing alpha to 0.0116
[2017.09.05-11:11:03] cost_eph# 3 = 0.0058; abs diff between current and last eph = 0.0005
[2017.09.05-11:11:03] eph# 3, gradient[380:385] = [  3.548279e-03  -9.772064e-04   6.691821e-06   1.078361e-05   1.056496e-05]
[2017.09.05-11:11:04] eph#4, cost decreased by 0.0003 ==> increasing alpha to 0.0122
[2017.09.05-11:11:04] eph#5, cost decreased by 0.0002 ==> increasing alpha to 0.0128
[2017.09.05-11:11:04] eph#6, cost decreased by 0.0002 ==> increasing alpha to 0.0134
[2017.09.05-11:11:04] cost_eph# 6 = 0.0051; abs diff between current and last eph = 0.0002
[2017.09.05-11:11:04] eph# 6, gradient[380:385] = [  3.049176e-03  -7.858201e-04  -6.326202e-06  -7.535287e-06  -4.993869e-06]
[2017.09.05-11:11:05] eph#7, cost decreased by 0.0002 ==> increasing alpha to 0.0141
[2017.09.05-11:11:05] eph#8, cost decreased by 0.0001 ==> increasing alpha to 0.0148
[2017.09.05-11:11:06] eph#9, cost decreased by 0.0001 ==> increasing alpha to 0.0155
[2017.09.05-11:11:06] cost_eph# 9 = 0.0047; abs diff between current and last eph = 0.0001
[2017.09.05-11:11:06] eph# 9, gradient[380:385] = [  2.562945e-03  -7.276213e-04  -8.097935e-06  -1.256236e-05  -9.778117e-06]
[2017.09.05-11:11:06] eph#10, cost decreased by 0.0001 ==> increasing alpha to 0.0163
[2017.09.05-11:11:06] eph#10, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:11:06] Time for simple with reg training = 4.559s
[2017.09.05-11:11:06] Computing theta for target = 2
[2017.09.05-11:11:06] Start simple with reg training: alpha=0.01, batchSz=10, beta=0.001
[2017.09.05-11:11:07] eph#1, cost decreased by 0.0030 ==> increasing alpha to 0.0105
[2017.09.05-11:11:07] eph#2, cost decreased by 0.0014 ==> increasing alpha to 0.0110
[2017.09.05-11:11:08] eph#3, cost decreased by 0.0008 ==> increasing alpha to 0.0116
[2017.09.05-11:11:08] cost_eph# 3 = 0.0122; abs diff between current and last eph = 0.0008
[2017.09.05-11:11:08] eph# 3, gradient[380:385] = [-0.008494  0.002789  0.007376  0.010127  0.010447]
[2017.09.05-11:11:08] eph#4, cost decreased by 0.0006 ==> increasing alpha to 0.0122
[2017.09.05-11:11:09] eph#5, cost decreased by 0.0004 ==> increasing alpha to 0.0128
[2017.09.05-11:11:09] eph#6, cost decreased by 0.0003 ==> increasing alpha to 0.0134
[2017.09.05-11:11:09] cost_eph# 6 = 0.0109; abs diff between current and last eph = 0.0003
[2017.09.05-11:11:09] eph# 6, gradient[380:385] = [-0.009132  0.00183   0.005553  0.008003  0.008104]
[2017.09.05-11:11:09] eph#7, cost decreased by 0.0003 ==> increasing alpha to 0.0141
[2017.09.05-11:11:10] eph#8, cost decreased by 0.0002 ==> increasing alpha to 0.0148
[2017.09.05-11:11:10] eph#9, cost decreased by 0.0002 ==> increasing alpha to 0.0155
[2017.09.05-11:11:10] cost_eph# 9 = 0.0101; abs diff between current and last eph = 0.0002
[2017.09.05-11:11:10] eph# 9, gradient[380:385] = [-0.009519  0.001248  0.004112  0.006306  0.006357]
[2017.09.05-11:11:11] eph#10, cost decreased by 0.0002 ==> increasing alpha to 0.0163
[2017.09.05-11:11:11] eph#11, cost decreased by 0.0002 ==> increasing alpha to 0.0171
[2017.09.05-11:11:11] eph#12, cost decreased by 0.0001 ==> increasing alpha to 0.0180
[2017.09.05-11:11:11] cost_eph#12 = 0.0096; abs diff between current and last eph = 0.0001
[2017.09.05-11:11:11] eph#12, gradient[380:385] = [-0.009793  0.000856  0.003083  0.005052  0.005083]
[2017.09.05-11:11:12] eph#13, cost decreased by 0.0001 ==> increasing alpha to 0.0189
[2017.09.05-11:11:12] eph#14, cost decreased by 0.0001 ==> increasing alpha to 0.0198
[2017.09.05-11:11:12] Time for simple with reg training = 6.260s
[2017.09.05-11:11:12] Computing theta for target = 3
[2017.09.05-11:11:12] Start simple with reg training: alpha=0.01, batchSz=10, beta=0.001
[2017.09.05-11:11:13] eph#1, cost decreased by 0.0029 ==> increasing alpha to 0.0105
[2017.09.05-11:11:14] eph#2, cost decreased by 0.0013 ==> increasing alpha to 0.0110
[2017.09.05-11:11:14] eph#3, cost decreased by 0.0008 ==> increasing alpha to 0.0116
[2017.09.05-11:11:14] cost_eph# 3 = 0.0140; abs diff between current and last eph = 0.0008
[2017.09.05-11:11:14] eph# 3, gradient[380:385] = [ 0.005109  0.002067  0.007656  0.007967  0.004404]
[2017.09.05-11:11:14] eph#4, cost decreased by 0.0006 ==> increasing alpha to 0.0122
[2017.09.05-11:11:15] eph#5, cost decreased by 0.0004 ==> increasing alpha to 0.0128
[2017.09.05-11:11:15] eph#6, cost decreased by 0.0004 ==> increasing alpha to 0.0134
[2017.09.05-11:11:15] cost_eph# 6 = 0.0127; abs diff between current and last eph = 0.0004
[2017.09.05-11:11:15] eph# 6, gradient[380:385] = [ 0.003775  0.001467  0.005512  0.005742  0.002972]
[2017.09.05-11:11:16] eph#7, cost decreased by 0.0003 ==> increasing alpha to 0.0141
[2017.09.05-11:11:16] eph#8, cost decreased by 0.0003 ==> increasing alpha to 0.0148
[2017.09.05-11:11:16] eph#9, cost decreased by 0.0002 ==> increasing alpha to 0.0155
[2017.09.05-11:11:16] cost_eph# 9 = 0.0119; abs diff between current and last eph = 0.0002
[2017.09.05-11:11:16] eph# 9, gradient[380:385] = [ 0.002973  0.001099  0.004021  0.004201  0.001979]
[2017.09.05-11:11:17] eph#10, cost decreased by 0.0002 ==> increasing alpha to 0.0163
[2017.09.05-11:11:17] eph#11, cost decreased by 0.0002 ==> increasing alpha to 0.0171
[2017.09.05-11:11:18] eph#12, cost decreased by 0.0002 ==> increasing alpha to 0.0180
[2017.09.05-11:11:18] cost_eph#12 = 0.0114; abs diff between current and last eph = 0.0002
[2017.09.05-11:11:18] eph#12, gradient[380:385] = [ 0.002432  0.000862  0.003051  0.003197  0.001347]
[2017.09.05-11:11:18] eph#13, cost decreased by 0.0001 ==> increasing alpha to 0.0189
[2017.09.05-11:11:18] eph#14, cost decreased by 0.0001 ==> increasing alpha to 0.0198
[2017.09.05-11:11:18] Time for simple with reg training = 6.009s
[2017.09.05-11:11:18] Computing theta for target = 4
[2017.09.05-11:11:18] Start simple with reg training: alpha=0.01, batchSz=10, beta=0.001
[2017.09.05-11:11:19] eph#1, cost decreased by 0.0031 ==> increasing alpha to 0.0105
[2017.09.05-11:11:19] eph#2, cost decreased by 0.0014 ==> increasing alpha to 0.0110
[2017.09.05-11:11:20] eph#3, cost decreased by 0.0008 ==> increasing alpha to 0.0116
[2017.09.05-11:11:20] cost_eph# 3 = 0.0111; abs diff between current and last eph = 0.0008
[2017.09.05-11:11:20] eph# 3, gradient[380:385] = [ 0.005793  0.009489  0.009472  0.011199  0.009035]
[2017.09.05-11:11:20] eph#4, cost decreased by 0.0006 ==> increasing alpha to 0.0122
[2017.09.05-11:11:21] eph#5, cost decreased by 0.0004 ==> increasing alpha to 0.0128
[2017.09.05-11:11:21] eph#6, cost decreased by 0.0003 ==> increasing alpha to 0.0134
[2017.09.05-11:11:21] cost_eph# 6 = 0.0097; abs diff between current and last eph = 0.0003
[2017.09.05-11:11:21] eph# 6, gradient[380:385] = [ 0.006813  0.011344  0.011406  0.012684  0.010092]
[2017.09.05-11:11:21] eph#7, cost decreased by 0.0003 ==> increasing alpha to 0.0141
[2017.09.05-11:11:22] eph#8, cost decreased by 0.0002 ==> increasing alpha to 0.0148
[2017.09.05-11:11:22] eph#9, cost decreased by 0.0002 ==> increasing alpha to 0.0155
[2017.09.05-11:11:22] cost_eph# 9 = 0.0091; abs diff between current and last eph = 0.0002
[2017.09.05-11:11:22] eph# 9, gradient[380:385] = [ 0.007454  0.0125    0.012617  0.013669  0.01081 ]
[2017.09.05-11:11:23] eph#10, cost decreased by 0.0002 ==> increasing alpha to 0.0163
[2017.09.05-11:11:23] eph#11, cost decreased by 0.0001 ==> increasing alpha to 0.0171
[2017.09.05-11:11:24] eph#12, cost decreased by 0.0001 ==> increasing alpha to 0.0180
[2017.09.05-11:11:24] cost_eph#12 = 0.0086; abs diff between current and last eph = 0.0001
[2017.09.05-11:11:24] eph#12, gradient[380:385] = [ 0.007849  0.013214  0.013368  0.014264  0.01124 ]
[2017.09.05-11:11:24] eph#13, cost decreased by 0.0001 ==> increasing alpha to 0.0189
[2017.09.05-11:11:24] eph#14, cost decreased by 0.0001 ==> increasing alpha to 0.0198
[2017.09.05-11:11:24] Time for simple with reg training = 6.023s
[2017.09.05-11:11:24] Computing theta for target = 5
[2017.09.05-11:11:24] Start simple with reg training: alpha=0.01, batchSz=10, beta=0.001
[2017.09.05-11:11:25] eph#1, cost decreased by 0.0039 ==> increasing alpha to 0.0105
[2017.09.05-11:11:26] eph#2, cost decreased by 0.0018 ==> increasing alpha to 0.0110
[2017.09.05-11:11:26] eph#3, cost decreased by 0.0011 ==> increasing alpha to 0.0116
[2017.09.05-11:11:26] cost_eph# 3 = 0.0164; abs diff between current and last eph = 0.0011
[2017.09.05-11:11:26] eph# 3, gradient[380:385] = [  5.924911e-04   1.758644e-04   6.520033e-06   2.251591e-04   3.302979e-04]
[2017.09.05-11:11:26] eph#4, cost decreased by 0.0007 ==> increasing alpha to 0.0122
[2017.09.05-11:11:27] eph#5, cost decreased by 0.0006 ==> increasing alpha to 0.0128
[2017.09.05-11:11:27] eph#6, cost decreased by 0.0005 ==> increasing alpha to 0.0134
[2017.09.05-11:11:27] cost_eph# 6 = 0.0147; abs diff between current and last eph = 0.0005
[2017.09.05-11:11:27] eph# 6, gradient[380:385] = [  3.255966e-04   8.075812e-05  -1.101134e-05   1.008341e-04   1.713092e-04]
[2017.09.05-11:11:28] eph#7, cost decreased by 0.0004 ==> increasing alpha to 0.0141
[2017.09.05-11:11:28] eph#8, cost decreased by 0.0003 ==> increasing alpha to 0.0148
[2017.09.05-11:11:28] eph#9, cost decreased by 0.0003 ==> increasing alpha to 0.0155
[2017.09.05-11:11:28] cost_eph# 9 = 0.0137; abs diff between current and last eph = 0.0003
[2017.09.05-11:11:28] eph# 9, gradient[380:385] = [  2.151860e-04   4.460354e-05  -1.689372e-05   5.546462e-05   1.113507e-04]
[2017.09.05-11:11:29] eph#10, cost decreased by 0.0002 ==> increasing alpha to 0.0163
[2017.09.05-11:11:29] eph#11, cost decreased by 0.0002 ==> increasing alpha to 0.0171
[2017.09.05-11:11:30] eph#12, cost decreased by 0.0002 ==> increasing alpha to 0.0180
[2017.09.05-11:11:30] cost_eph#12 = 0.0131; abs diff between current and last eph = 0.0002
[2017.09.05-11:11:30] eph#12, gradient[380:385] = [  1.582641e-04   2.600833e-05  -2.013313e-05   3.441535e-05   8.286871e-05]
[2017.09.05-11:11:30] eph#13, cost decreased by 0.0002 ==> increasing alpha to 0.0189
[2017.09.05-11:11:30] eph#14, cost decreased by 0.0002 ==> increasing alpha to 0.0198
[2017.09.05-11:11:30] Time for simple with reg training = 5.990s
[2017.09.05-11:11:30] Computing theta for target = 6
[2017.09.05-11:11:30] Start simple with reg training: alpha=0.01, batchSz=10, beta=0.001
[2017.09.05-11:11:31] eph#1, cost decreased by 0.0024 ==> increasing alpha to 0.0105
[2017.09.05-11:11:32] eph#2, cost decreased by 0.0010 ==> increasing alpha to 0.0110
[2017.09.05-11:11:32] eph#3, cost decreased by 0.0006 ==> increasing alpha to 0.0116
[2017.09.05-11:11:32] cost_eph# 3 = 0.0079; abs diff between current and last eph = 0.0006
[2017.09.05-11:11:32] eph# 3, gradient[380:385] = [-0.001801 -0.003172 -0.003172 -0.002585 -0.001992]
[2017.09.05-11:11:32] eph#4, cost decreased by 0.0004 ==> increasing alpha to 0.0122
[2017.09.05-11:11:33] eph#5, cost decreased by 0.0003 ==> increasing alpha to 0.0128
[2017.09.05-11:11:33] eph#6, cost decreased by 0.0002 ==> increasing alpha to 0.0134
[2017.09.05-11:11:33] cost_eph# 6 = 0.0070; abs diff between current and last eph = 0.0002
[2017.09.05-11:11:33] eph# 6, gradient[380:385] = [-0.001508 -0.002627 -0.002675 -0.002362 -0.001807]
[2017.09.05-11:11:33] eph#7, cost decreased by 0.0002 ==> increasing alpha to 0.0141
[2017.09.05-11:11:34] eph#8, cost decreased by 0.0002 ==> increasing alpha to 0.0148
[2017.09.05-11:11:34] eph#9, cost decreased by 0.0001 ==> increasing alpha to 0.0155
[2017.09.05-11:11:34] cost_eph# 9 = 0.0065; abs diff between current and last eph = 0.0001
[2017.09.05-11:11:34] eph# 9, gradient[380:385] = [-0.001318 -0.002283 -0.002339 -0.00213  -0.001627]
[2017.09.05-11:11:35] eph#10, cost decreased by 0.0001 ==> increasing alpha to 0.0163
[2017.09.05-11:11:35] eph#11, cost decreased by 0.0001 ==> increasing alpha to 0.0171
[2017.09.05-11:11:35] eph#12, cost decreased by 0.0001 ==> increasing alpha to 0.0180
[2017.09.05-11:11:35] eph#12, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:11:35] Time for simple with reg training = 5.159s
[2017.09.05-11:11:35] Computing theta for target = 7
[2017.09.05-11:11:35] Start simple with reg training: alpha=0.01, batchSz=10, beta=0.001
[2017.09.05-11:11:36] eph#1, cost decreased by 0.0021 ==> increasing alpha to 0.0105
[2017.09.05-11:11:37] eph#2, cost decreased by 0.0009 ==> increasing alpha to 0.0110
[2017.09.05-11:11:37] eph#3, cost decreased by 0.0005 ==> increasing alpha to 0.0116
[2017.09.05-11:11:37] cost_eph# 3 = 0.0089; abs diff between current and last eph = 0.0005
[2017.09.05-11:11:37] eph# 3, gradient[380:385] = [ 0.002612  0.001062 -0.003501 -0.000864 -0.000972]
[2017.09.05-11:11:38] eph#4, cost decreased by 0.0004 ==> increasing alpha to 0.0122
[2017.09.05-11:11:38] eph#5, cost decreased by 0.0003 ==> increasing alpha to 0.0128
[2017.09.05-11:11:38] eph#6, cost decreased by 0.0002 ==> increasing alpha to 0.0134
[2017.09.05-11:11:38] cost_eph# 6 = 0.0080; abs diff between current and last eph = 0.0002
[2017.09.05-11:11:38] eph# 6, gradient[380:385] = [ 0.00271   0.001313 -0.002573 -0.000584 -0.000658]
[2017.09.05-11:11:39] eph#7, cost decreased by 0.0002 ==> increasing alpha to 0.0141
[2017.09.05-11:11:39] eph#8, cost decreased by 0.0002 ==> increasing alpha to 0.0148
[2017.09.05-11:11:40] eph#9, cost decreased by 0.0001 ==> increasing alpha to 0.0155
[2017.09.05-11:11:40] cost_eph# 9 = 0.0075; abs diff between current and last eph = 0.0001
[2017.09.05-11:11:40] eph# 9, gradient[380:385] = [ 0.002654  0.001353 -0.002185 -0.000582 -0.000639]
[2017.09.05-11:11:40] eph#10, cost decreased by 0.0001 ==> increasing alpha to 0.0163
[2017.09.05-11:11:40] eph#11, cost decreased by 0.0001 ==> increasing alpha to 0.0171
[2017.09.05-11:11:41] eph#12, cost decreased by 0.0001 ==> increasing alpha to 0.0180
[2017.09.05-11:11:41] eph#12, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:11:41] Time for simple with reg training = 5.227s
[2017.09.05-11:11:41] Computing theta for target = 8
[2017.09.05-11:11:41] Start simple with reg training: alpha=0.01, batchSz=10, beta=0.001
[2017.09.05-11:11:42] eph#1, cost decreased by 0.0044 ==> increasing alpha to 0.0105
[2017.09.05-11:11:42] eph#2, cost decreased by 0.0020 ==> increasing alpha to 0.0110
[2017.09.05-11:11:42] eph#3, cost decreased by 0.0012 ==> increasing alpha to 0.0116
[2017.09.05-11:11:42] cost_eph# 3 = 0.0227; abs diff between current and last eph = 0.0012
[2017.09.05-11:11:42] eph# 3, gradient[380:385] = [ 0.006853  0.001381  0.000721  0.001149  0.001055]
[2017.09.05-11:11:43] eph#4, cost decreased by 0.0009 ==> increasing alpha to 0.0122
[2017.09.05-11:11:43] eph#5, cost decreased by 0.0007 ==> increasing alpha to 0.0128
[2017.09.05-11:11:43] eph#6, cost decreased by 0.0006 ==> increasing alpha to 0.0134
[2017.09.05-11:11:43] cost_eph# 6 = 0.0206; abs diff between current and last eph = 0.0006
[2017.09.05-11:11:43] eph# 6, gradient[380:385] = [ 0.005743  0.000958  0.000456  0.000695  0.000643]
[2017.09.05-11:11:44] eph#7, cost decreased by 0.0005 ==> increasing alpha to 0.0141
[2017.09.05-11:11:44] eph#8, cost decreased by 0.0004 ==> increasing alpha to 0.0148
[2017.09.05-11:11:45] eph#9, cost decreased by 0.0004 ==> increasing alpha to 0.0155
[2017.09.05-11:11:45] cost_eph# 9 = 0.0193; abs diff between current and last eph = 0.0004
[2017.09.05-11:11:45] eph# 9, gradient[380:385] = [ 0.004869  0.000773  0.000361  0.000509  0.000472]
[2017.09.05-11:11:45] eph#10, cost decreased by 0.0003 ==> increasing alpha to 0.0163
[2017.09.05-11:11:45] eph#11, cost decreased by 0.0003 ==> increasing alpha to 0.0171
[2017.09.05-11:11:46] eph#12, cost decreased by 0.0003 ==> increasing alpha to 0.0180
[2017.09.05-11:11:46] cost_eph#12 = 0.0184; abs diff between current and last eph = 0.0003
[2017.09.05-11:11:46] eph#12, gradient[380:385] = [ 0.004083  0.000654  0.000313  0.000408  0.000378]
[2017.09.05-11:11:46] eph#13, cost decreased by 0.0002 ==> increasing alpha to 0.0189
[2017.09.05-11:11:47] eph#14, cost decreased by 0.0002 ==> increasing alpha to 0.0198
[2017.09.05-11:11:47] Time for simple with reg training = 5.929s
[2017.09.05-11:11:47] Computing theta for target = 9
[2017.09.05-11:11:47] Start simple with reg training: alpha=0.01, batchSz=10, beta=0.001
[2017.09.05-11:11:47] eph#1, cost decreased by 0.0040 ==> increasing alpha to 0.0105
[2017.09.05-11:11:48] eph#2, cost decreased by 0.0018 ==> increasing alpha to 0.0110
[2017.09.05-11:11:48] eph#3, cost decreased by 0.0011 ==> increasing alpha to 0.0116
[2017.09.05-11:11:48] cost_eph# 3 = 0.0189; abs diff between current and last eph = 0.0011
[2017.09.05-11:11:48] eph# 3, gradient[380:385] = [ 0.001754  0.003161  0.004596 -0.00391  -0.004235]
[2017.09.05-11:11:49] eph#4, cost decreased by 0.0008 ==> increasing alpha to 0.0122
[2017.09.05-11:11:49] eph#5, cost decreased by 0.0006 ==> increasing alpha to 0.0128
[2017.09.05-11:11:49] eph#6, cost decreased by 0.0005 ==> increasing alpha to 0.0134
[2017.09.05-11:11:49] cost_eph# 6 = 0.0170; abs diff between current and last eph = 0.0005
[2017.09.05-11:11:49] eph# 6, gradient[380:385] = [ 0.001798  0.003243  0.00437  -0.002657 -0.003087]
[2017.09.05-11:11:50] eph#7, cost decreased by 0.0004 ==> increasing alpha to 0.0141
[2017.09.05-11:11:50] eph#8, cost decreased by 0.0003 ==> increasing alpha to 0.0148
[2017.09.05-11:11:51] eph#9, cost decreased by 0.0003 ==> increasing alpha to 0.0155
[2017.09.05-11:11:51] cost_eph# 9 = 0.0160; abs diff between current and last eph = 0.0003
[2017.09.05-11:11:51] eph# 9, gradient[380:385] = [ 0.00185   0.003301  0.004192 -0.002158 -0.002645]
[2017.09.05-11:11:51] eph#10, cost decreased by 0.0002 ==> increasing alpha to 0.0163
[2017.09.05-11:11:51] eph#11, cost decreased by 0.0002 ==> increasing alpha to 0.0171
[2017.09.05-11:11:52] eph#12, cost decreased by 0.0002 ==> increasing alpha to 0.0180
[2017.09.05-11:11:52] cost_eph#12 = 0.0154; abs diff between current and last eph = 0.0002
[2017.09.05-11:11:52] eph#12, gradient[380:385] = [ 0.001881  0.003322  0.004032 -0.002037 -0.002553]
[2017.09.05-11:11:52] eph#13, cost decreased by 0.0002 ==> increasing alpha to 0.0189
[2017.09.05-11:11:53] eph#14, cost decreased by 0.0002 ==> increasing alpha to 0.0198
[2017.09.05-11:11:53] Time for simple with reg training = 5.948s
[2017.09.05-11:11:53] Total train time for simple with reg 55.823s
[2017.09.05-11:11:53] Results using simple with reg solver -- test
[2017.09.05-11:11:53] General accuracy results are: correct=8882, wrong=919, accuracy=90.62%
[2017.09.05-11:11:53] Printing results for target 0: correct=930, wrong=28, accuracy=97.08%
[2017.09.05-11:11:53] Printing results for target 1: correct=1072, wrong=28, accuracy=97.45%
[2017.09.05-11:11:53] Printing results for target 2: correct=865, wrong=117, accuracy=88.09%
[2017.09.05-11:11:53] Printing results for target 3: correct=874, wrong=114, accuracy=88.46%
[2017.09.05-11:11:53] Printing results for target 4: correct=840, wrong=67, accuracy=92.61%
[2017.09.05-11:11:53] Printing results for target 5: correct=756, wrong=147, accuracy=83.72%
[2017.09.05-11:11:53] Printing results for target 6: correct=974, wrong=37, accuracy=96.34%
[2017.09.05-11:11:53] Printing results for target 7: correct=959, wrong=94, accuracy=91.07%
[2017.09.05-11:11:53] Printing results for target 8: correct=827, wrong=136, accuracy=85.88%
[2017.09.05-11:11:53] Printing results for target 9: correct=785, wrong=151, accuracy=83.87%
[2017.09.05-11:11:53] Best accuracy is 97.45% for digit 1
[2017.09.05-11:11:53] Worst accuracy is 83.72% for digit 5
[2017.09.05-11:12:23] Results using simple with reg solver -- train
[2017.09.05-11:12:23] General accuracy results are: correct=54388, wrong=5811, accuracy=90.35%
[2017.09.05-11:12:23] Printing results for target 0: correct=5782, wrong=163, accuracy=97.26%
[2017.09.05-11:12:23] Printing results for target 1: correct=6552, wrong=225, accuracy=96.68%
[2017.09.05-11:12:23] Printing results for target 2: correct=5239, wrong=769, accuracy=87.20%
[2017.09.05-11:12:23] Printing results for target 3: correct=5409, wrong=744, accuracy=87.91%
[2017.09.05-11:12:23] Printing results for target 4: correct=5431, wrong=486, accuracy=91.79%
[2017.09.05-11:12:23] Printing results for target 5: correct=4435, wrong=975, accuracy=81.98%
[2017.09.05-11:12:23] Printing results for target 6: correct=5578, wrong=287, accuracy=95.11%
[2017.09.05-11:12:23] Printing results for target 7: correct=5719, wrong=521, accuracy=91.65%
[2017.09.05-11:12:23] Printing results for target 8: correct=4980, wrong=882, accuracy=84.95%
[2017.09.05-11:12:23] Printing results for target 9: correct=5263, wrong=759, accuracy=87.40%
[2017.09.05-11:12:23] Best accuracy is 97.26% for digit 0
[2017.09.05-11:12:23] Worst accuracy is 81.98% for digit 5
[2017.09.05-11:12:31] Initialize momentun gradient descendent logisitic regression solver
[2017.09.05-11:12:31] Computing theta for target = 0
[2017.09.05-11:12:31] Start momentum without reg training: alpha=0.001, batchSz=10, beta=0, momentum=0.9
[2017.09.05-11:12:32] eph#1, cost decreased by 0.0020 ==> increasing alpha to 0.0010
[2017.09.05-11:12:32] eph#2, cost decreased by 0.0009 ==> increasing alpha to 0.0010
[2017.09.05-11:12:32] eph#3, cost decreased by 0.0005 ==> increasing alpha to 0.0010
[2017.09.05-11:12:32] cost_eph# 3 = 0.0060; abs diff between current and last eph = 0.0005
[2017.09.05-11:12:32] eph# 3, gradient[380:385] = [  5.183288e-04   5.661449e-05   2.331877e-05   7.248800e-05  -8.259406e-05]
[2017.09.05-11:12:33] eph#4, cost decreased by 0.0004 ==> increasing alpha to 0.0010
[2017.09.05-11:12:33] eph#5, cost decreased by 0.0003 ==> increasing alpha to 0.0010
[2017.09.05-11:12:34] eph#6, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:12:34] cost_eph# 6 = 0.0052; abs diff between current and last eph = 0.0002
[2017.09.05-11:12:34] eph# 6, gradient[380:385] = [  3.647820e-04   3.317085e-05   1.531137e-05   3.763739e-05  -4.265920e-05]
[2017.09.05-11:12:34] eph#7, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:12:35] eph#8, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:35] eph#9, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:35] cost_eph# 9 = 0.0048; abs diff between current and last eph = 0.0001
[2017.09.05-11:12:35] eph# 9, gradient[380:385] = [  2.860346e-04   2.373978e-05   1.156634e-05   2.486851e-05  -3.140988e-05]
[2017.09.05-11:12:35] eph#10, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:35] eph#10, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:12:35] Time for momentum without reg training = 4.701s
[2017.09.05-11:12:35] Computing theta for target = 1
[2017.09.05-11:12:35] Start momentum without reg training: alpha=0.001, batchSz=10, beta=0, momentum=0.9
[2017.09.05-11:12:36] eph#1, cost decreased by 0.0018 ==> increasing alpha to 0.0010
[2017.09.05-11:12:37] eph#2, cost decreased by 0.0007 ==> increasing alpha to 0.0010
[2017.09.05-11:12:37] eph#3, cost decreased by 0.0004 ==> increasing alpha to 0.0010
[2017.09.05-11:12:37] cost_eph# 3 = 0.0058; abs diff between current and last eph = 0.0004
[2017.09.05-11:12:37] eph# 3, gradient[380:385] = [  3.489259e-03  -9.761751e-04   2.119334e-05   2.458663e-05   1.992076e-05]
[2017.09.05-11:12:37] eph#4, cost decreased by 0.0003 ==> increasing alpha to 0.0010
[2017.09.05-11:12:38] eph#5, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:12:38] eph#6, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:12:38] cost_eph# 6 = 0.0051; abs diff between current and last eph = 0.0002
[2017.09.05-11:12:38] eph# 6, gradient[380:385] = [  3.052358e-03  -7.818708e-04   7.554808e-06   8.389694e-06   6.696515e-06]
[2017.09.05-11:12:39] eph#7, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:39] eph#8, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:40] eph#9, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:40] cost_eph# 9 = 0.0048; abs diff between current and last eph = 0.0001
[2017.09.05-11:12:40] eph# 9, gradient[380:385] = [  2.640059e-03  -7.103207e-04   3.878021e-06   4.196609e-06   3.323760e-06]
[2017.09.05-11:12:40] eph#10, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:40] eph#10, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:12:40] Time for momentum without reg training = 4.614s
[2017.09.05-11:12:40] Computing theta for target = 2
[2017.09.05-11:12:40] Start momentum without reg training: alpha=0.001, batchSz=10, beta=0, momentum=0.9
[2017.09.05-11:12:41] eph#1, cost decreased by 0.0031 ==> increasing alpha to 0.0010
[2017.09.05-11:12:41] eph#2, cost decreased by 0.0013 ==> increasing alpha to 0.0010
[2017.09.05-11:12:42] eph#3, cost decreased by 0.0008 ==> increasing alpha to 0.0010
[2017.09.05-11:12:42] cost_eph# 3 = 0.0122; abs diff between current and last eph = 0.0008
[2017.09.05-11:12:42] eph# 3, gradient[380:385] = [-0.00846   0.002845  0.007519  0.010281  0.010587]
[2017.09.05-11:12:42] eph#4, cost decreased by 0.0005 ==> increasing alpha to 0.0010
[2017.09.05-11:12:42] eph#5, cost decreased by 0.0004 ==> increasing alpha to 0.0010
[2017.09.05-11:12:43] eph#6, cost decreased by 0.0003 ==> increasing alpha to 0.0010
[2017.09.05-11:12:43] cost_eph# 6 = 0.0110; abs diff between current and last eph = 0.0003
[2017.09.05-11:12:43] eph# 6, gradient[380:385] = [-0.009047  0.001969  0.005943  0.008447  0.008535]
[2017.09.05-11:12:43] eph#7, cost decreased by 0.0003 ==> increasing alpha to 0.0010
[2017.09.05-11:12:44] eph#8, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:12:44] eph#9, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:12:44] cost_eph# 9 = 0.0103; abs diff between current and last eph = 0.0002
[2017.09.05-11:12:44] eph# 9, gradient[380:385] = [-0.009386  0.001453  0.004719  0.007023  0.007059]
[2017.09.05-11:12:45] eph#10, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:12:45] eph#11, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:45] eph#12, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:45] cost_eph#12 = 0.0099; abs diff between current and last eph = 0.0001
[2017.09.05-11:12:45] eph#12, gradient[380:385] = [-0.009621  0.001109  0.003828  0.005972  0.005985]
[2017.09.05-11:12:46] eph#13, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:46] eph#14, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:46] Time for momentum without reg training = 6.285s
[2017.09.05-11:12:46] Computing theta for target = 3
[2017.09.05-11:12:46] Start momentum without reg training: alpha=0.001, batchSz=10, beta=0, momentum=0.9
[2017.09.05-11:12:47] eph#1, cost decreased by 0.0029 ==> increasing alpha to 0.0010
[2017.09.05-11:12:48] eph#2, cost decreased by 0.0013 ==> increasing alpha to 0.0010
[2017.09.05-11:12:48] eph#3, cost decreased by 0.0008 ==> increasing alpha to 0.0010
[2017.09.05-11:12:48] cost_eph# 3 = 0.0141; abs diff between current and last eph = 0.0008
[2017.09.05-11:12:48] eph# 3, gradient[380:385] = [ 0.005118  0.002058  0.007622  0.007935  0.004416]
[2017.09.05-11:12:48] eph#4, cost decreased by 0.0005 ==> increasing alpha to 0.0010
[2017.09.05-11:12:49] eph#5, cost decreased by 0.0004 ==> increasing alpha to 0.0010
[2017.09.05-11:12:49] eph#6, cost decreased by 0.0003 ==> increasing alpha to 0.0010
[2017.09.05-11:12:49] cost_eph# 6 = 0.0128; abs diff between current and last eph = 0.0003
[2017.09.05-11:12:49] eph# 6, gradient[380:385] = [ 0.003871  0.001499  0.005658  0.005896  0.003131]
[2017.09.05-11:12:50] eph#7, cost decreased by 0.0003 ==> increasing alpha to 0.0010
[2017.09.05-11:12:50] eph#8, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:12:50] eph#9, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:12:50] cost_eph# 9 = 0.0121; abs diff between current and last eph = 0.0002
[2017.09.05-11:12:50] eph# 9, gradient[380:385] = [ 0.003148  0.001161  0.004305  0.004497  0.00225 ]
[2017.09.05-11:12:51] eph#10, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:12:51] eph#11, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:12:52] eph#12, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:52] cost_eph#12 = 0.0117; abs diff between current and last eph = 0.0001
[2017.09.05-11:12:52] eph#12, gradient[380:385] = [ 0.002661  0.00094   0.003396  0.003556  0.001667]
[2017.09.05-11:12:52] eph#13, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:53] eph#14, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:53] Time for momentum without reg training = 6.508s
[2017.09.05-11:12:53] Computing theta for target = 4
[2017.09.05-11:12:53] Start momentum without reg training: alpha=0.001, batchSz=10, beta=0, momentum=0.9
[2017.09.05-11:12:54] eph#1, cost decreased by 0.0031 ==> increasing alpha to 0.0010
[2017.09.05-11:12:54] eph#2, cost decreased by 0.0014 ==> increasing alpha to 0.0010
[2017.09.05-11:12:54] eph#3, cost decreased by 0.0008 ==> increasing alpha to 0.0010
[2017.09.05-11:12:54] cost_eph# 3 = 0.0111; abs diff between current and last eph = 0.0008
[2017.09.05-11:12:54] eph# 3, gradient[380:385] = [ 0.005716  0.009351  0.009336  0.011052  0.008918]
[2017.09.05-11:12:55] eph#4, cost decreased by 0.0005 ==> increasing alpha to 0.0010
[2017.09.05-11:12:55] eph#5, cost decreased by 0.0004 ==> increasing alpha to 0.0010
[2017.09.05-11:12:56] eph#6, cost decreased by 0.0003 ==> increasing alpha to 0.0010
[2017.09.05-11:12:56] cost_eph# 6 = 0.0098; abs diff between current and last eph = 0.0003
[2017.09.05-11:12:56] eph# 6, gradient[380:385] = [ 0.006642  0.011043  0.011103  0.012391  0.009866]
[2017.09.05-11:12:56] eph#7, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:12:57] eph#8, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:12:57] eph#9, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:12:57] cost_eph# 9 = 0.0092; abs diff between current and last eph = 0.0002
[2017.09.05-11:12:57] eph# 9, gradient[380:385] = [ 0.007223  0.012095  0.012206  0.013287  0.010518]
[2017.09.05-11:12:57] eph#10, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:58] eph#11, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:58] eph#12, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:58] cost_eph#12 = 0.0088; abs diff between current and last eph = 0.0001
[2017.09.05-11:12:58] eph#12, gradient[380:385] = [ 0.007603  0.012783  0.01293   0.013876  0.010947]
[2017.09.05-11:12:59] eph#13, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:59] eph#14, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:12:59] eph#14, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:12:59] Time for momentum without reg training = 6.290s
[2017.09.05-11:12:59] Computing theta for target = 5
[2017.09.05-11:12:59] Start momentum without reg training: alpha=0.001, batchSz=10, beta=0, momentum=0.9
[2017.09.05-11:13:00] eph#1, cost decreased by 0.0039 ==> increasing alpha to 0.0010
[2017.09.05-11:13:00] eph#2, cost decreased by 0.0017 ==> increasing alpha to 0.0010
[2017.09.05-11:13:01] eph#3, cost decreased by 0.0010 ==> increasing alpha to 0.0010
[2017.09.05-11:13:01] cost_eph# 3 = 0.0165; abs diff between current and last eph = 0.0010
[2017.09.05-11:13:01] eph# 3, gradient[380:385] = [  6.094530e-04   1.902491e-04   2.162388e-05   2.374859e-04   3.423372e-04]
[2017.09.05-11:13:01] eph#4, cost decreased by 0.0007 ==> increasing alpha to 0.0010
[2017.09.05-11:13:02] eph#5, cost decreased by 0.0005 ==> increasing alpha to 0.0010
[2017.09.05-11:13:02] eph#6, cost decreased by 0.0004 ==> increasing alpha to 0.0010
[2017.09.05-11:13:02] cost_eph# 6 = 0.0149; abs diff between current and last eph = 0.0004
[2017.09.05-11:13:02] eph# 6, gradient[380:385] = [  3.552281e-04   1.012685e-04   7.367831e-06   1.169022e-04   1.863820e-04]
[2017.09.05-11:13:02] eph#7, cost decreased by 0.0003 ==> increasing alpha to 0.0010
[2017.09.05-11:13:03] eph#8, cost decreased by 0.0003 ==> increasing alpha to 0.0010
[2017.09.05-11:13:03] eph#9, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:13:03] cost_eph# 9 = 0.0140; abs diff between current and last eph = 0.0002
[2017.09.05-11:13:03] eph# 9, gradient[380:385] = [  2.481361e-04   6.771139e-05   3.688269e-06   7.167958e-05   1.247314e-04]
[2017.09.05-11:13:04] eph#10, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:13:04] eph#11, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:13:04] eph#12, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:13:04] cost_eph#12 = 0.0134; abs diff between current and last eph = 0.0002
[2017.09.05-11:13:04] eph#12, gradient[380:385] = [  1.905364e-04   5.037953e-05   2.235169e-06   4.944257e-05   9.294317e-05]
[2017.09.05-11:13:05] eph#13, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:13:05] eph#14, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:13:05] Time for momentum without reg training = 6.286s
[2017.09.05-11:13:05] Computing theta for target = 6
[2017.09.05-11:13:05] Start momentum without reg training: alpha=0.001, batchSz=10, beta=0, momentum=0.9
[2017.09.05-11:13:06] eph#1, cost decreased by 0.0024 ==> increasing alpha to 0.0010
[2017.09.05-11:13:07] eph#2, cost decreased by 0.0010 ==> increasing alpha to 0.0010
[2017.09.05-11:13:07] eph#3, cost decreased by 0.0006 ==> increasing alpha to 0.0010
[2017.09.05-11:13:07] cost_eph# 3 = 0.0079; abs diff between current and last eph = 0.0006
[2017.09.05-11:13:07] eph# 3, gradient[380:385] = [-0.001776 -0.003136 -0.003131 -0.002542 -0.00196 ]
[2017.09.05-11:13:07] eph#4, cost decreased by 0.0004 ==> increasing alpha to 0.0010
[2017.09.05-11:13:08] eph#5, cost decreased by 0.0003 ==> increasing alpha to 0.0010
[2017.09.05-11:13:08] eph#6, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:13:08] cost_eph# 6 = 0.0071; abs diff between current and last eph = 0.0002
[2017.09.05-11:13:08] eph# 6, gradient[380:385] = [-0.001497 -0.002615 -0.002655 -0.002332 -0.001786]
[2017.09.05-11:13:09] eph#7, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:13:09] eph#8, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:13:10] eph#9, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:13:10] cost_eph# 9 = 0.0066; abs diff between current and last eph = 0.0001
[2017.09.05-11:13:10] eph# 9, gradient[380:385] = [-0.001326 -0.002306 -0.002353 -0.002131 -0.001631]
[2017.09.05-11:13:10] eph#10, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:13:10] eph#11, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:13:10] eph#11, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:13:10] Time for momentum without reg training = 5.156s
[2017.09.05-11:13:10] Computing theta for target = 7
[2017.09.05-11:13:10] Start momentum without reg training: alpha=0.001, batchSz=10, beta=0, momentum=0.9
[2017.09.05-11:13:11] eph#1, cost decreased by 0.0021 ==> increasing alpha to 0.0010
[2017.09.05-11:13:12] eph#2, cost decreased by 0.0009 ==> increasing alpha to 0.0010
[2017.09.05-11:13:12] eph#3, cost decreased by 0.0005 ==> increasing alpha to 0.0010
[2017.09.05-11:13:12] cost_eph# 3 = 0.0089; abs diff between current and last eph = 0.0005
[2017.09.05-11:13:12] eph# 3, gradient[380:385] = [ 0.002608  0.001056 -0.003458 -0.000817 -0.000927]
[2017.09.05-11:13:13] eph#4, cost decreased by 0.0004 ==> increasing alpha to 0.0010
[2017.09.05-11:13:13] eph#5, cost decreased by 0.0003 ==> increasing alpha to 0.0010
[2017.09.05-11:13:13] eph#6, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:13:13] cost_eph# 6 = 0.0080; abs diff between current and last eph = 0.0002
[2017.09.05-11:13:13] eph# 6, gradient[380:385] = [ 0.002722  0.001315 -0.002528 -0.000492 -0.000571]
[2017.09.05-11:13:14] eph#7, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:13:14] eph#8, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:13:15] eph#9, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:13:15] cost_eph# 9 = 0.0076; abs diff between current and last eph = 0.0001
[2017.09.05-11:13:15] eph# 9, gradient[380:385] = [ 0.002715  0.001392 -0.002116 -0.000432 -0.000496]
[2017.09.05-11:13:15] eph#10, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:13:15] eph#11, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:13:15] eph#11, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:13:15] Time for momentum without reg training = 4.964s
[2017.09.05-11:13:15] Computing theta for target = 8
[2017.09.05-11:13:15] Start momentum without reg training: alpha=0.001, batchSz=10, beta=0, momentum=0.9
[2017.09.05-11:13:16] eph#1, cost decreased by 0.0044 ==> increasing alpha to 0.0010
[2017.09.05-11:13:17] eph#2, cost decreased by 0.0019 ==> increasing alpha to 0.0010
[2017.09.05-11:13:17] eph#3, cost decreased by 0.0011 ==> increasing alpha to 0.0010
[2017.09.05-11:13:17] cost_eph# 3 = 0.0228; abs diff between current and last eph = 0.0011
[2017.09.05-11:13:17] eph# 3, gradient[380:385] = [ 0.006858  0.001366  0.00071   0.001141  0.001046]
[2017.09.05-11:13:18] eph#4, cost decreased by 0.0008 ==> increasing alpha to 0.0010
[2017.09.05-11:13:18] eph#5, cost decreased by 0.0006 ==> increasing alpha to 0.0010
[2017.09.05-11:13:18] eph#6, cost decreased by 0.0005 ==> increasing alpha to 0.0010
[2017.09.05-11:13:18] cost_eph# 6 = 0.0208; abs diff between current and last eph = 0.0005
[2017.09.05-11:13:18] eph# 6, gradient[380:385] = [ 0.00591   0.000963  0.000453  0.000711  0.000656]
[2017.09.05-11:13:19] eph#7, cost decreased by 0.0004 ==> increasing alpha to 0.0010
[2017.09.05-11:13:19] eph#8, cost decreased by 0.0004 ==> increasing alpha to 0.0010
[2017.09.05-11:13:20] eph#9, cost decreased by 0.0003 ==> increasing alpha to 0.0010
[2017.09.05-11:13:20] cost_eph# 9 = 0.0198; abs diff between current and last eph = 0.0003
[2017.09.05-11:13:20] eph# 9, gradient[380:385] = [ 0.005256  0.000789  0.000354  0.000531  0.000492]
[2017.09.05-11:13:20] eph#10, cost decreased by 0.0003 ==> increasing alpha to 0.0010
[2017.09.05-11:13:20] eph#11, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:13:21] eph#12, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:13:21] cost_eph#12 = 0.0190; abs diff between current and last eph = 0.0002
[2017.09.05-11:13:21] eph#12, gradient[380:385] = [ 0.004716  0.000684  0.000303  0.000432  0.0004  ]
[2017.09.05-11:13:21] eph#13, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:13:22] eph#14, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:13:22] Time for momentum without reg training = 6.218s
[2017.09.05-11:13:22] Computing theta for target = 9
[2017.09.05-11:13:22] Start momentum without reg training: alpha=0.001, batchSz=10, beta=0, momentum=0.9
[2017.09.05-11:13:23] eph#1, cost decreased by 0.0040 ==> increasing alpha to 0.0010
[2017.09.05-11:13:23] eph#2, cost decreased by 0.0018 ==> increasing alpha to 0.0010
[2017.09.05-11:13:23] eph#3, cost decreased by 0.0011 ==> increasing alpha to 0.0010
[2017.09.05-11:13:23] cost_eph# 3 = 0.0190; abs diff between current and last eph = 0.0011
[2017.09.05-11:13:23] eph# 3, gradient[380:385] = [ 0.001722  0.003115  0.004548 -0.003991 -0.004308]
[2017.09.05-11:13:24] eph#4, cost decreased by 0.0007 ==> increasing alpha to 0.0010
[2017.09.05-11:13:24] eph#5, cost decreased by 0.0006 ==> increasing alpha to 0.0010
[2017.09.05-11:13:25] eph#6, cost decreased by 0.0004 ==> increasing alpha to 0.0010
[2017.09.05-11:13:25] cost_eph# 6 = 0.0172; abs diff between current and last eph = 0.0004
[2017.09.05-11:13:25] eph# 6, gradient[380:385] = [ 0.001732  0.003146  0.004291 -0.002839 -0.003247]
[2017.09.05-11:13:25] eph#7, cost decreased by 0.0004 ==> increasing alpha to 0.0010
[2017.09.05-11:13:25] eph#8, cost decreased by 0.0003 ==> increasing alpha to 0.0010
[2017.09.05-11:13:26] eph#9, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:13:26] cost_eph# 9 = 0.0163; abs diff between current and last eph = 0.0002
[2017.09.05-11:13:26] eph# 9, gradient[380:385] = [ 0.001756  0.003167  0.004104 -0.002328 -0.002783]
[2017.09.05-11:13:26] eph#10, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:13:27] eph#11, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:13:27] eph#12, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:13:27] cost_eph#12 = 0.0158; abs diff between current and last eph = 0.0002
[2017.09.05-11:13:27] eph#12, gradient[380:385] = [ 0.00177   0.003166  0.003946 -0.002103 -0.002584]
[2017.09.05-11:13:28] eph#13, cost decreased by 0.0002 ==> increasing alpha to 0.0010
[2017.09.05-11:13:28] eph#14, cost decreased by 0.0001 ==> increasing alpha to 0.0010
[2017.09.05-11:13:28] Time for momentum without reg training = 6.587s
[2017.09.05-11:13:28] Total train time for momentun without reg 57.610s
[2017.09.05-11:13:28] Results using momentun without reg solver -- test
[2017.09.05-11:13:28] General accuracy results are: correct=8864, wrong=937, accuracy=90.44%
[2017.09.05-11:13:28] Printing results for target 0: correct=929, wrong=29, accuracy=96.97%
[2017.09.05-11:13:28] Printing results for target 1: correct=1072, wrong=28, accuracy=97.45%
[2017.09.05-11:13:28] Printing results for target 2: correct=864, wrong=118, accuracy=87.98%
[2017.09.05-11:13:28] Printing results for target 3: correct=875, wrong=113, accuracy=88.56%
[2017.09.05-11:13:28] Printing results for target 4: correct=839, wrong=68, accuracy=92.50%
[2017.09.05-11:13:28] Printing results for target 5: correct=753, wrong=150, accuracy=83.39%
[2017.09.05-11:13:28] Printing results for target 6: correct=973, wrong=38, accuracy=96.24%
[2017.09.05-11:13:28] Printing results for target 7: correct=957, wrong=96, accuracy=90.88%
[2017.09.05-11:13:28] Printing results for target 8: correct=819, wrong=144, accuracy=85.05%
[2017.09.05-11:13:28] Printing results for target 9: correct=783, wrong=153, accuracy=83.65%
[2017.09.05-11:13:28] Best accuracy is 97.45% for digit 1
[2017.09.05-11:13:28] Worst accuracy is 83.39% for digit 5
[2017.09.05-11:14:52] Results using momentun without reg solver -- train
[2017.09.05-11:14:52] General accuracy results are: correct=54215, wrong=5984, accuracy=90.06%
[2017.09.05-11:14:52] Printing results for target 0: correct=5776, wrong=169, accuracy=97.16%
[2017.09.05-11:14:52] Printing results for target 1: correct=6560, wrong=217, accuracy=96.80%
[2017.09.05-11:14:52] Printing results for target 2: correct=5220, wrong=788, accuracy=86.88%
[2017.09.05-11:14:52] Printing results for target 3: correct=5378, wrong=775, accuracy=87.40%
[2017.09.05-11:14:52] Printing results for target 4: correct=5418, wrong=499, accuracy=91.57%
[2017.09.05-11:14:52] Printing results for target 5: correct=4406, wrong=1004, accuracy=81.44%
[2017.09.05-11:14:52] Printing results for target 6: correct=5566, wrong=299, accuracy=94.90%
[2017.09.05-11:14:52] Printing results for target 7: correct=5705, wrong=535, accuracy=91.43%
[2017.09.05-11:14:52] Printing results for target 8: correct=4945, wrong=917, accuracy=84.36%
[2017.09.05-11:14:52] Printing results for target 9: correct=5241, wrong=781, accuracy=87.03%
[2017.09.05-11:14:52] Best accuracy is 97.16% for digit 0
[2017.09.05-11:14:52] Worst accuracy is 81.44% for digit 5
[2017.09.05-11:14:57] Initialize momentun gradient descendent logisitic regression solver
[2017.09.05-11:14:57] Computing theta for target = 0
[2017.09.05-11:14:57] Start momentum with reg training: alpha=0.01, batchSz=10, beta=0.001, momentum=0.9
[2017.09.05-11:14:58] eph#1, cost decreased by 0.0006 ==> increasing alpha to 0.0100
[2017.09.05-11:14:58] eph#2, cost decreased by 0.0003 ==> increasing alpha to 0.0100
[2017.09.05-11:14:59] eph#3, cost decreased by 0.0002 ==> increasing alpha to 0.0100
[2017.09.05-11:14:59] cost_eph# 3 = 0.0039; abs diff between current and last eph = 0.0002
[2017.09.05-11:14:59] eph# 3, gradient[380:385] = [  1.525806e-04  -3.342802e-05  -1.498691e-05   4.472502e-06  -1.532578e-05]
[2017.09.05-11:14:59] eph#4, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:00] eph#5, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:00] eph#5, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:15:00] Time for momentum with reg training = 2.461s
[2017.09.05-11:15:00] Computing theta for target = 1
[2017.09.05-11:15:00] Start momentum with reg training: alpha=0.01, batchSz=10, beta=0.001, momentum=0.9
[2017.09.05-11:15:01] eph#1, cost decreased by 0.0005 ==> increasing alpha to 0.0100
[2017.09.05-11:15:01] eph#2, cost decreased by 0.0003 ==> increasing alpha to 0.0100
[2017.09.05-11:15:01] eph#3, cost decreased by 0.0002 ==> increasing alpha to 0.0100
[2017.09.05-11:15:01] cost_eph# 3 = 0.0039; abs diff between current and last eph = 0.0002
[2017.09.05-11:15:01] eph# 3, gradient[380:385] = [  1.106650e-03  -7.624963e-04  -8.921258e-07  -1.775545e-05  -1.774116e-05]
[2017.09.05-11:15:02] eph#4, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:02] eph#5, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:02] eph#5, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:15:02] Time for momentum with reg training = 2.610s
[2017.09.05-11:15:02] Computing theta for target = 2
[2017.09.05-11:15:02] Start momentum with reg training: alpha=0.01, batchSz=10, beta=0.001, momentum=0.9
[2017.09.05-11:15:03] eph#1, cost decreased by 0.0010 ==> increasing alpha to 0.0100
[2017.09.05-11:15:04] eph#2, cost decreased by 0.0004 ==> increasing alpha to 0.0100
[2017.09.05-11:15:04] eph#3, cost decreased by 0.0003 ==> increasing alpha to 0.0100
[2017.09.05-11:15:04] cost_eph# 3 = 0.0087; abs diff between current and last eph = 0.0003
[2017.09.05-11:15:04] eph# 3, gradient[380:385] = [ -1.039783e-02   9.294169e-05   1.234321e-03   2.468404e-03   2.481970e-03]
[2017.09.05-11:15:04] eph#4, cost decreased by 0.0002 ==> increasing alpha to 0.0100
[2017.09.05-11:15:05] eph#5, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:05] eph#6, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:05] eph#6, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:15:05] Time for momentum with reg training = 2.924s
[2017.09.05-11:15:05] Computing theta for target = 3
[2017.09.05-11:15:05] Start momentum with reg training: alpha=0.01, batchSz=10, beta=0.001, momentum=0.9
[2017.09.05-11:15:06] eph#1, cost decreased by 0.0011 ==> increasing alpha to 0.0100
[2017.09.05-11:15:07] eph#2, cost decreased by 0.0005 ==> increasing alpha to 0.0100
[2017.09.05-11:15:07] eph#3, cost decreased by 0.0003 ==> increasing alpha to 0.0100
[2017.09.05-11:15:07] cost_eph# 3 = 0.0104; abs diff between current and last eph = 0.0003
[2017.09.05-11:15:07] eph# 3, gradient[380:385] = [ 0.001794  0.000575  0.002137  0.002264  0.000585]
[2017.09.05-11:15:07] eph#4, cost decreased by 0.0002 ==> increasing alpha to 0.0100
[2017.09.05-11:15:08] eph#5, cost decreased by 0.0002 ==> increasing alpha to 0.0100
[2017.09.05-11:15:08] eph#6, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:08] cost_eph# 6 = 0.0099; abs diff between current and last eph = 0.0001
[2017.09.05-11:15:08] eph# 6, gradient[380:385] = [ 0.001393  0.000422  0.001424  0.001509  0.000323]
[2017.09.05-11:15:09] eph#7, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:09] eph#7, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:15:09] Time for momentum with reg training = 3.357s
[2017.09.05-11:15:09] Computing theta for target = 4
[2017.09.05-11:15:09] Start momentum with reg training: alpha=0.01, batchSz=10, beta=0.001, momentum=0.9
[2017.09.05-11:15:09] eph#1, cost decreased by 0.0010 ==> increasing alpha to 0.0100
[2017.09.05-11:15:10] eph#2, cost decreased by 0.0004 ==> increasing alpha to 0.0100
[2017.09.05-11:15:10] eph#3, cost decreased by 0.0002 ==> increasing alpha to 0.0100
[2017.09.05-11:15:10] cost_eph# 3 = 0.0078; abs diff between current and last eph = 0.0002
[2017.09.05-11:15:10] eph# 3, gradient[380:385] = [ 0.007885  0.013345  0.013549  0.014007  0.010954]
[2017.09.05-11:15:11] eph#4, cost decreased by 0.0002 ==> increasing alpha to 0.0100
[2017.09.05-11:15:11] eph#5, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:12] eph#6, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:12] eph#6, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:15:12] Time for momentum with reg training = 2.954s
[2017.09.05-11:15:12] Computing theta for target = 5
[2017.09.05-11:15:12] Start momentum with reg training: alpha=0.01, batchSz=10, beta=0.001, momentum=0.9
[2017.09.05-11:15:12] eph#1, cost decreased by 0.0013 ==> increasing alpha to 0.0100
[2017.09.05-11:15:13] eph#2, cost decreased by 0.0006 ==> increasing alpha to 0.0100
[2017.09.05-11:15:13] eph#3, cost decreased by 0.0004 ==> increasing alpha to 0.0100
[2017.09.05-11:15:13] cost_eph# 3 = 0.0118; abs diff between current and last eph = 0.0004
[2017.09.05-11:15:13] eph# 3, gradient[380:385] = [  9.123580e-05   3.862528e-06  -2.598001e-05   1.092054e-05   4.557454e-05]
[2017.09.05-11:15:14] eph#4, cost decreased by 0.0003 ==> increasing alpha to 0.0100
[2017.09.05-11:15:14] eph#5, cost decreased by 0.0002 ==> increasing alpha to 0.0100
[2017.09.05-11:15:15] eph#6, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:15] cost_eph# 6 = 0.0112; abs diff between current and last eph = 0.0001
[2017.09.05-11:15:15] eph# 6, gradient[380:385] = [  8.564580e-05  -8.188744e-07  -2.919603e-05   7.895340e-06   4.451749e-05]
[2017.09.05-11:15:15] eph#7, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:15] eph#8, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:15] eph#8, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:15:15] Time for momentum with reg training = 3.788s
[2017.09.05-11:15:15] Computing theta for target = 6
[2017.09.05-11:15:15] Start momentum with reg training: alpha=0.01, batchSz=10, beta=0.001, momentum=0.9
[2017.09.05-11:15:16] eph#1, cost decreased by 0.0007 ==> increasing alpha to 0.0100
[2017.09.05-11:15:17] eph#2, cost decreased by 0.0003 ==> increasing alpha to 0.0100
[2017.09.05-11:15:17] eph#3, cost decreased by 0.0002 ==> increasing alpha to 0.0100
[2017.09.05-11:15:17] cost_eph# 3 = 0.0056; abs diff between current and last eph = 0.0002
[2017.09.05-11:15:17] eph# 3, gradient[380:385] = [-0.000569 -0.000986 -0.001022 -0.000929 -0.000703]
[2017.09.05-11:15:17] eph#4, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:18] eph#5, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:18] eph#5, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:15:18] Time for momentum with reg training = 2.529s
[2017.09.05-11:15:18] Computing theta for target = 7
[2017.09.05-11:15:18] Start momentum with reg training: alpha=0.01, batchSz=10, beta=0.001, momentum=0.9
[2017.09.05-11:15:19] eph#1, cost decreased by 0.0006 ==> increasing alpha to 0.0100
[2017.09.05-11:15:19] eph#2, cost decreased by 0.0003 ==> increasing alpha to 0.0100
[2017.09.05-11:15:20] eph#3, cost decreased by 0.0002 ==> increasing alpha to 0.0100
[2017.09.05-11:15:20] cost_eph# 3 = 0.0067; abs diff between current and last eph = 0.0002
[2017.09.05-11:15:20] eph# 3, gradient[380:385] = [ 0.001666  0.000751 -0.001891 -0.001063 -0.001094]
[2017.09.05-11:15:20] eph#4, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:20] eph#5, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:20] eph#5, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:15:20] Time for momentum with reg training = 2.512s
[2017.09.05-11:15:20] Computing theta for target = 8
[2017.09.05-11:15:20] Start momentum with reg training: alpha=0.01, batchSz=10, beta=0.001, momentum=0.9
[2017.09.05-11:15:21] eph#1, cost decreased by 0.0019 ==> increasing alpha to 0.0100
[2017.09.05-11:15:22] eph#2, cost decreased by 0.0009 ==> increasing alpha to 0.0100
[2017.09.05-11:15:22] eph#3, cost decreased by 0.0005 ==> increasing alpha to 0.0100
[2017.09.05-11:15:22] cost_eph# 3 = 0.0167; abs diff between current and last eph = 0.0005
[2017.09.05-11:15:22] eph# 3, gradient[380:385] = [ 0.001955  0.00039   0.000226  0.000241  0.000211]
[2017.09.05-11:15:22] eph#4, cost decreased by 0.0003 ==> increasing alpha to 0.0100
[2017.09.05-11:15:23] eph#5, cost decreased by 0.0002 ==> increasing alpha to 0.0100
[2017.09.05-11:15:23] eph#6, cost decreased by 0.0002 ==> increasing alpha to 0.0100
[2017.09.05-11:15:23] cost_eph# 6 = 0.0159; abs diff between current and last eph = 0.0002
[2017.09.05-11:15:23] eph# 6, gradient[380:385] = [ 0.00142   0.000311  0.000197  0.000202  0.000175]
[2017.09.05-11:15:24] eph#7, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:24] eph#8, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:24] eph#8, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:15:24] Time for momentum with reg training = 3.771s
[2017.09.05-11:15:24] Computing theta for target = 9
[2017.09.05-11:15:24] Start momentum with reg training: alpha=0.01, batchSz=10, beta=0.001, momentum=0.9
[2017.09.05-11:15:25] eph#1, cost decreased by 0.0013 ==> increasing alpha to 0.0100
[2017.09.05-11:15:25] eph#2, cost decreased by 0.0006 ==> increasing alpha to 0.0100
[2017.09.05-11:15:26] eph#3, cost decreased by 0.0004 ==> increasing alpha to 0.0100
[2017.09.05-11:15:26] cost_eph# 3 = 0.0142; abs diff between current and last eph = 0.0004
[2017.09.05-11:15:26] eph# 3, gradient[380:385] = [ 0.002321  0.004011  0.004432 -0.001005 -0.001727]
[2017.09.05-11:15:26] eph#4, cost decreased by 0.0002 ==> increasing alpha to 0.0100
[2017.09.05-11:15:27] eph#5, cost decreased by 0.0002 ==> increasing alpha to 0.0100
[2017.09.05-11:15:27] eph#6, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:27] cost_eph# 6 = 0.0137; abs diff between current and last eph = 0.0001
[2017.09.05-11:15:27] eph# 6, gradient[380:385] = [ 0.002169  0.00372   0.004005 -0.002136 -0.002784]
[2017.09.05-11:15:28] eph#7, cost decreased by 0.0001 ==> increasing alpha to 0.0100
[2017.09.05-11:15:28] eph#7, delta(cost) < epsilon ==> early stopping
[2017.09.05-11:15:28] Time for momentum with reg training = 3.335s
[2017.09.05-11:15:28] Total train time for momentun with reg 30.240s
[2017.09.05-11:15:28] Results using momentun with reg solver -- test
[2017.09.05-11:15:28] General accuracy results are: correct=8959, wrong=842, accuracy=91.41%
[2017.09.05-11:15:28] Printing results for target 0: correct=935, wrong=23, accuracy=97.60%
[2017.09.05-11:15:28] Printing results for target 1: correct=1074, wrong=26, accuracy=97.64%
[2017.09.05-11:15:28] Printing results for target 2: correct=875, wrong=107, accuracy=89.10%
[2017.09.05-11:15:28] Printing results for target 3: correct=885, wrong=103, accuracy=89.57%
[2017.09.05-11:15:28] Printing results for target 4: correct=839, wrong=68, accuracy=92.50%
[2017.09.05-11:15:28] Printing results for target 5: correct=779, wrong=124, accuracy=86.27%
[2017.09.05-11:15:28] Printing results for target 6: correct=972, wrong=39, accuracy=96.14%
[2017.09.05-11:15:28] Printing results for target 7: correct=967, wrong=86, accuracy=91.83%
[2017.09.05-11:15:28] Printing results for target 8: correct=836, wrong=127, accuracy=86.81%
[2017.09.05-11:15:28] Printing results for target 9: correct=797, wrong=139, accuracy=85.15%
[2017.09.05-11:15:28] Best accuracy is 97.64% for digit 1
[2017.09.05-11:15:28] Worst accuracy is 85.15% for digit 9
[2017.09.05-11:15:31] Results using momentun with reg solver -- train
[2017.09.05-11:15:31] General accuracy results are: correct=54956, wrong=5243, accuracy=91.29%
[2017.09.05-11:15:31] Printing results for target 0: correct=5818, wrong=127, accuracy=97.86%
[2017.09.05-11:15:31] Printing results for target 1: correct=6566, wrong=211, accuracy=96.89%
[2017.09.05-11:15:31] Printing results for target 2: correct=5324, wrong=684, accuracy=88.62%
[2017.09.05-11:15:31] Printing results for target 3: correct=5501, wrong=652, accuracy=89.40%
[2017.09.05-11:15:31] Printing results for target 4: correct=5451, wrong=466, accuracy=92.12%
[2017.09.05-11:15:31] Printing results for target 5: correct=4579, wrong=831, accuracy=84.64%
[2017.09.05-11:15:31] Printing results for target 6: correct=5606, wrong=259, accuracy=95.58%
[2017.09.05-11:15:31] Printing results for target 7: correct=5748, wrong=492, accuracy=92.12%
[2017.09.05-11:15:31] Printing results for target 8: correct=5050, wrong=812, accuracy=86.15%
[2017.09.05-11:15:31] Printing results for target 9: correct=5313, wrong=709, accuracy=88.23%
[2017.09.05-11:15:31] Best accuracy is 97.86% for digit 0
[2017.09.05-11:15:31] Worst accuracy is 84.64% for digit 5
[2017.09.05-11:15:37] Summary of general results:

     Alg    Reg  TestAcc  TrainAcc  BestTestAcc  WorstTestAcc  BestTrainAcc  WorstTrainAcc  TotalTrainTime
0   SGD  False    90.71     90.40        97.45         83.72         97.22          82.14          54.714
1   SGD   True    90.62     90.35        97.45         83.72         97.26          81.98          55.823
2  MSGD  False    90.44     90.06        97.45         83.39         97.16          81.44          57.610
3  MSGD   True    91.41     91.29        97.64         85.15         97.86          84.64          30.240
